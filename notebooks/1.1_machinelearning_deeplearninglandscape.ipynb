{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marcoteran/deeplearning/blob/master/notebooks/1.1_machinelearning_deeplearninglandscape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Abrir en Colab\" title=\"Abrir y ejecutar en Google Colaboratory\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/marcoteran/deeplearning/blob/master/notebooks/1.1_machinelearning_deeplearninglandscape.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Abrir en Kaggle\" title=\"Abrir y ejecutar en Kaggle\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de c√≥digo\n",
    "# Sesi√≥n 01: Proyecto integral de Machine Learning\n",
    "## Deep Learning y series de tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** Marco Teran\n",
    "**E-mail:** marco.teran@usa.edu.co\n",
    "\n",
    "[Website](http://marcoteran.github.io/),\n",
    "[Github](https://github.com/marcoteran),\n",
    "[LinkedIn](https://www.linkedin.com/in/marcoteran/).\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"¬°Bienvenidos al primer notebook!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este proyecto requiere Python 3.7 o superior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "assert sys.version_info >= (3, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also requires Scikit-Learn ‚â• 1.0.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging import version\n",
    "import sklearn\n",
    "\n",
    "print(version.parse(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtener los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bienvenido a Machine Learning Housing Corp. Su tarea es predecir el valor medio de la vivienda en los distritos de California, dada una serie de caracter√≠sticas de estos distritos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descargar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "def load_housing_data():\n",
    "    tarball_path = Path(\"datasets/housing.tgz\")\n",
    "    if not tarball_path.is_file():\n",
    "        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n",
    "        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n",
    "        urllib.request.urlretrieve(url, tarball_path)\n",
    "        with tarfile.open(tarball_path) as housing_tarball:\n",
    "            housing_tarball.extractall(path=\"datasets\")\n",
    "    return pd.read_csv(Path(\"datasets/housing/housing.csv\"))\n",
    "\n",
    "housing = load_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eche un vistazo a la estructura de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para empezar, se observan las cinco primeras filas de datos mediante el m√©todo head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada fila representa un distrito. Hay 10 atributos (no se muestran todos en la captura de pantalla): longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, median_house_value, y ocean_proximity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El m√©todo info() es √∫til para obtener una descripci√≥n r√°pida de los datos, en particular el n√∫mero total de filas, el tipo de cada atributo y el n√∫mero de valores no nulos. no nulos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay 20.640 instancias en el conjunto de datos, lo que significa que es bastante peque√±o para los est√°ndares de aprendizaje autom√°tico, pero es perfecto para empezar. Observe que el atributo total_habitaciones s√≥lo tiene 20.433 valores no nulos, lo que significa que 207 distritos carecen de esta caracter√≠stica. Tendr√° que ocuparse de esto m√°s adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos los atributos son num√©ricos, excepto ocean_proximity.\n",
    "El tipo de ocean_proximity es objeto, pero como se cargaron los datos desde un archivo CSV, se sabe que es un atributo de texto.\n",
    "Al mirar las cinco filas superiores, se not√≥ que los valores de la columna ocean_proximity eran repetitivos, lo que sugiere que se trata de un atributo categ√≥rico.\n",
    "Se puede usar el m√©todo value_counts() para averiguar qu√© categor√≠as existen y cu√°ntos distritos pertenecen a cada categor√≠a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos los dem√°s campos. El m√©todo describe() muestra un resumen de los atributos num√©ricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente celda tampoco se muestra en el libro. Crea la carpeta `images/end_to_end_project` (si no existe ya), y define la funci√≥n `save_fig()` que se utiliza a trav√©s de este cuaderno para guardar las figuras en alta resoluci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c√≥digo extra - c√≥digo para guardar las figuras como PNG de alta resoluci√≥n para el libro\n",
    "\n",
    "IMAGES_PATH = Path() / \"images\" / \"end_to_end_project\"\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una forma r√°pida de entender el tipo de datos con los que est√°s trabajando es graficar un histograma para cada atributo num√©rico. Un histograma muestra el n√∫mero de instancias (en el eje vertical) que tienen un rango de valores dado (en el eje horizontal). Puedes graficar un solo atributo a la vez, o puedes llamar al m√©todo hist() en todo el conjunto de datos (como se muestra en el siguiente ejemplo de c√≥digo), y generar√° un histograma para cada atributo num√©rico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# c√≥digo extra - las siguientes 5 l√≠neas definen los tama√±os de fuente por defecto\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "\n",
    "housing.hist(bins=50, figsize=(12, 8))\n",
    "save_fig(\"attribute_histogram_plots\")  # extra code\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear un conjunto de pruebas (Test Set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es recomendable reservar parte de los datos para formar un conjunto de prueba. Esto se debe a que si se mira el conjunto de prueba, se puede caer en la trampa del sobreajuste, y seleccionar un modelo que no funcionar√° tan bien como se esperaba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear un conjunto de prueba es te√≥ricamente simple; elige algunas instancias al azar, t√≠picamente el 20% del conjunto de datos (o menos si tu conjunto de datos es muy grande), y res√©rvalos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def shuffle_and_split_data(data, test_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = shuffle_and_split_data(housing, 0.2)\n",
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para asegurarnos de que los resultados de este cuaderno son los mismos cada vez que lo ejecutamos, necesitamos establecer la semilla aleatoria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lamentablemente, esto no garantiza que este cuaderno produzca exactamente los mismos resultados que el libro, ya que hay otras posibles fuentes de variaci√≥n. La m√°s importante es el hecho de que los algoritmos se modifican con el tiempo cuando las bibliotecas evolucionan. As√≠ que, por favor, tolera algunas peque√±as diferencias: con suerte, la mayor√≠a de los resultados deber√≠an ser los mismos, o al menos aproximados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: otra fuente de aleatoriedad es el orden de los conjuntos de Python: se basa en la funci√≥n `hash()` de Python, que es \"salada\" aleatoriamente cuando Python arranca (esto empez√≥ en Python 3.3, para prevenir algunos ataques de denegaci√≥n de servicio). Para eliminar esta aleatoriedad, la soluci√≥n es establecer la variable de entorno `PYTHONHASHSEED` a `\"0\"` _antes_ de que Python se inicie. No pasar√° nada si lo haces despu√©s. Afortunadamente, si est√° ejecutando este cuaderno en Colab, la variable ya est√° configurada para usted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zlib import crc32\n",
    "\n",
    "def is_id_in_test_set(identifier, test_ratio):\n",
    "    return crc32(np.int64(identifier)) < test_ratio * 2**32\n",
    "\n",
    "def split_data_with_id_hash(data, test_ratio, id_column):\n",
    "    ids = data[id_column]\n",
    "    in_test_set = ids.apply(lambda id_: is_id_in_test_set(id_, test_ratio))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lamentablemente, el conjunto de datos sobre vivienda no dispone de una columna de identificadores. La soluci√≥n m√°s sencilla de soluci√≥n m√°s sencilla es utilizar el √≠ndice de la fila como identificador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_with_id = housing.reset_index()  # adds an `index` column\n",
    "train_set, test_set = split_data_with_id_hash(housing_with_id, 0.2, \"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se utiliza el √≠ndice de filas como identificador √∫nico, hay que asegurarse de que que los nuevos datos se a√±adan al final del conjunto de datos y que nunca se fila. Si esto no es posible, puede intentar utilizar las caracter√≠sticas m√°s estables para construir un identificador √∫nico. Por ejemplo, est√° garantizado que la latitud y la longitud de un distrito se mantendr√°n estables durante varios millones de a√±os:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\n",
    "train_set, test_set = split_data_with_id_hash(housing_with_id, 0.2, \"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn proporciona algunas funciones para dividir conjuntos de datos en m√∫ltiples subconjuntos de varias formas. La funci√≥n m√°s simple es train_test_split(), que hace pr√°cticamente lo mismo que la funci√≥n shuffle_and_split_data() que definimos anteriormente, pero con un par de caracter√≠sticas adicionales. Primero, hay un par√°metro random_state que te permite establecer la semilla del generador aleatorio. En segundo lugar, puedes pasarle varios conjuntos de datos con un n√∫mero id√©ntico de filas y los dividir√° en los mismos √≠ndices (esto es muy √∫til, por ejemplo, si tienes un DataFrame separado para etiquetas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set[\"total_bedrooms\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hallar la probabilidad de que una muestra aleatoria de 1.000 personas contenga menos del 48,5% de mujeres o m√°s del 53,5% de mujeres cuando la proporci√≥n de mujeres de la poblaci√≥n es del 51,1%, utilizamos la [distribuci√≥n binomial](https://en.wikipedia.org/wiki/Binomial_distribution). El m√©todo `cdf()` de la distribuci√≥n binomial nos da la probabilidad de que el n√∫mero de mujeres sea igual o menor que el valor dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c√≥digo adicional - muestra c√≥mo calcular la probabilidad del 10,7% de obtener una muestra err√≥nea\n",
    "\n",
    "from scipy.stats import binom\n",
    "\n",
    "sample_size = 1000\n",
    "ratio_female = 0.511\n",
    "proba_too_small = binom(sample_size, ratio_female).cdf(485 - 1)\n",
    "proba_too_large = 1 - binom(sample_size, ratio_female).cdf(535)\n",
    "print(proba_too_small + proba_too_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si prefieres las simulaciones a las matem√°ticas, aqu√≠ tienes c√≥mo obtener aproximadamente el mismo resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c√≥digo extra - muestra otra forma de estimar la probabilidad de mala muestra\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "samples = (np.random.rand(100_000, sample_size) < ratio_female).sum(axis=1)\n",
    "((samples < 485) | (samples > 535)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"income_cat\"].value_counts().sort_index().plot.bar(rot=0, grid=True)\n",
    "plt.xlabel(\"Categor√≠a de ingresos\")\n",
    "plt.ylabel(\"N√∫mero de distritos\")\n",
    "save_fig(\"housing_income_cat_bar_plot\") # c√≥digo adicional\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ser precisos, el m√©todo split() proporciona los √≠ndices de entrenamiento y prueba, no los datos en s√≠. Tener m√∫ltiples divisiones puede ser √∫til si desea estimar mejor el rendimiento de su modelo, como ver√° cuando hablemos de la validaci√≥n cruzada m√°s adelante en este cap√≠tulo. Por ejemplo, el siguiente c√≥digo genera 10 divisiones estratificadas diferentes del mismo conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "strat_splits = []\n",
    "for train_index, test_index in splitter.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set_n = housing.iloc[train_index]\n",
    "    strat_test_set_n = housing.iloc[test_index]\n",
    "    strat_splits.append([strat_train_set_n, strat_test_set_n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ahora, s√≥lo puedes utilizar la primera divisi√≥n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_set, strat_test_set = strat_splits[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puesto que el muestreo estratificado es bastante com√∫n, hay una forma m√°s corta de obtener una utilizando la funci√≥n train_test_split() con el argumento stratify. Es mucho m√°s corto obtener una √∫nica divisi√≥n estratificada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_set, strat_test_set = train_test_split(\n",
    "    housing, test_size=0.2, stratify=housing[\"income_cat\"], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos si ha funcionado como se esperaba. Para empezar, observe las proporciones de en el conjunto de pruebas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con un c√≥digo similar, se puede medir la proporci√≥n de categor√≠as de ingresos en el conjunto de datos completo. La tabla compara las proporciones de categor√≠as de ingresos en el conjunto de datos completo, en el conjunto de prueba generado con muestreo estratificado y en un conjunto de prueba generado utilizando un muestreo completamente aleatorio. Como se puede ver, el conjunto de prueba generado utilizando muestreo estratificado tiene proporciones de categor√≠as de ingresos casi id√©nticas a las del conjunto de datos completo, mientras que el conjunto de prueba generado utilizando muestreo completamente aleatorio est√° sesgado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def income_cat_proportions(data):\n",
    "    return data[\"income_cat\"].value_counts() / len(data)\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "\n",
    "compare_props = pd.DataFrame({\n",
    "    \"Overall %\": income_cat_proportions(housing),\n",
    "    \"Stratified %\": income_cat_proportions(strat_test_set),\n",
    "    \"Random %\": income_cat_proportions(test_set),\n",
    "}).sort_index()\n",
    "compare_props.index.name = \"Income Category\"\n",
    "compare_props[\"Strat. Error %\"] = (compare_props[\"Stratified %\"] /\n",
    "                                   compare_props[\"Overall %\"] - 1)\n",
    "compare_props[\"Rand. Error %\"] = (compare_props[\"Random %\"] /\n",
    "                                  compare_props[\"Overall %\"] - 1)\n",
    "(compare_props * 100).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No volver√°s a utilizar la columna income_cat, as√≠ que ser√° mejor que la elimines, volviendo los datos a su estado original: for set_ in (strat_train_set, strat_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dedicamos bastante tiempo a la generaci√≥n de conjuntos de pruebas por una buena raz√≥n: se trata de una parte a menudo descuidada pero fundamental de un proyecto de aprendizaje autom√°tico. parte cr√≠tica de un proyecto de aprendizaje autom√°tico. Adem√°s, muchas de estas ideas ser√°n √∫tiles m√°s adelante cuando hablemos de la validaci√≥n cruzada.\n",
    "Ahora es el momento de pasar a la siguiente etapa: explorar los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descubrir y visualizar los datos para obtener informaci√≥n (insights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es necesario una exploraci√≥n m√°s detallada de los datos y sugiere tomar medidas espec√≠ficas antes de explorar los datos en profundidad. Primero, se recomienda que separe el conjunto de prueba y solo explore el conjunto de entrenamiento. Adem√°s, si el conjunto de entrenamiento es grande, puede ser √∫til muestrear un conjunto de exploraci√≥n para facilitar y acelerar la exploraci√≥n. En este caso, el conjunto de entrenamiento es peque√±o, por lo que se puede trabajar directamente con el conjunto completo. Dado que se experimentar√° con varias transformaciones en el conjunto de entrenamiento completo, se sugiere hacer una copia del original para poder volver a √©l m√°s tarde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizaci√≥n de datos geogr√°ficos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que el conjunto de datos incluye informaci√≥n geogr√°fica (latitud y longitud), conviene crear un diagrama de dispersi√≥n de todos los distritos para visualizar los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True)\n",
    "save_fig(\"bad_visualization_plot\")  # extra code\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto se parece a California, pero aparte de eso es dif√≠cil ver un patr√≥n particular. patr√≥n particular. Establecer la opci√≥n alfa a 0,2 hace que sea mucho m√°s f√°cil de visualizar los lugares donde hay una alta densidad de puntos de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True, alpha=0.2)\n",
    "save_fig(\"better_visualization_plot\")  # extra code\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuaci√≥n, observa los precios de la vivienda. El radio de cada c√≠rculo representa la poblaci√≥n del distrito (opci√≥n s), y el color representa el precio (opci√≥n c). Aqu√≠ se utiliza un mapa de colores predefinido (opci√≥n cmap) llamado jet, que va del azul (valores bajos) al rojo (precios altos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True,\n",
    "             s=housing[\"population\"] / 100, label=\"population\",\n",
    "             c=\"median_house_value\", cmap=\"jet\", colorbar=True,\n",
    "             legend=True, sharex=False, figsize=(10, 7))\n",
    "save_fig(\"housing_prices_scatterplot\")  # extra code\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El argumento `sharex=False` corrige un error de visualizaci√≥n: sin √©l, los valores del eje x y la etiqueta no se muestran (v√©ase: https://github.com/pandas-dev/pandas/issues/10611)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La imagen muestra que los precios de las viviendas est√°n relacionados con la ubicaci√≥n y la densidad de poblaci√≥n. Un algoritmo de agrupaci√≥n podr√≠a ayudar a detectar el conglomerado principal y agregar nuevas caracter√≠sticas que midan la proximidad a los centros de los conglomerados. El atributo de proximidad al oc√©ano tambi√©n puede ser √∫til, pero en el norte de California los precios de la vivienda en los distritos costeros no son demasiado altos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buscando Correlaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como el conjunto de datos no es demasiado grande, puede calcular f√°cilmente el coeficiente de correlaci√≥n est√°ndar (tambi√©n llamado r de Pearson) entre cada par de atributos utilizando el m√©todo corr():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora puede ver en qu√© medida cada atributo est√° correlacionado con la mediana de la vivienda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado indica que el coeficiente de correlaci√≥n oscila entre -1 y 1, donde valores cercanos a 1 indican una fuerte correlaci√≥n positiva y valores cercanos a -1 indican una fuerte correlaci√≥n negativa. En el caso de la mediana de los ingresos y el valor medio de la vivienda, hay una correlaci√≥n positiva, mientras que hay una peque√±a correlaci√≥n negativa entre la latitud y el valor medio de la vivienda. Si el coeficiente se acerca a 0, significa que no hay correlaci√≥n lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprobar la correlaci√≥n entre los atributos, se puede utilizar la funci√≥n Pandas scatter_matrix(), que comparar√≠a cada atributo num√©rico con cada otro atributo num√©rico. Dado que hay 11 atributos num√©ricos, se deben centrar en algunos atributos que parezcan estar m√°s correlacionados con el valor medio de la vivienda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
    "              \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(12, 8))\n",
    "save_fig(\"scatter_matrix_plot\")  # extra code\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La diagonal principal estar√≠a llena de l√≠neas rectas si Pandas trazara cada contra s√≠ misma, lo que no ser√≠a muy √∫til. As√≠ que en su lugar, Pandas muestra un histograma de cada atributo (hay otras opciones disponibles; consulte la documentaci√≥n de Pandas para m√°s detalles)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando los diagramas de correlaci√≥n, parece que el atributo m√°s prometedor para predecir el valor medio de la vivienda es la renta media. para predecir el valor medio de la casa es la mediana de los ingresos, por lo que se ampliar su diagrama de dispersi√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n",
    "             alpha=0.1, grid=True)\n",
    "save_fig(\"income_vs_house_value_scatterplot\")  # extra code\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with Attribute Combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ha explorado los datos para identificar peculiaridades y correlaciones entre atributos antes de alimentar los datos a un algoritmo de aprendizaje autom√°tico. Tambi√©n se ha observado que algunos atributos tienen una distribuci√≥n sesgada a la derecha y se sugiere transformarlos para preparar los datos para los algoritmos de aprendizaje autom√°tico. Adem√°s, se ha recomendado probar varias combinaciones de atributos antes de preparar los datos y se sugieren nuevos atributos como el n√∫mero de habitaciones por hogar y la poblaci√≥n por hogar. Se deja planteado c√≥mo se crear√°n estos nuevos atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"rooms_per_house\"] = housing[\"total_rooms\"] / housing[\"households\"]\n",
    "housing[\"bedrooms_ratio\"] = housing[\"total_bedrooms\"] / housing[\"total_rooms\"]\n",
    "housing[\"people_per_house\"] = housing[\"population\"] / housing[\"households\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente la matriz de correlaci√≥n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparar los datos para los algoritmos de aprendizaje autom√°tico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Volver a un conjunto de entraniemnto limpio (copiando otra vez `strat_train_set`)\n",
    "* Separar los predictores y las etiquetas (no es necesario aplicar las mismas transformaciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvamos al conjunto de entrenamiento original y separemos el objetivo (ten en cuenta que `strat_train_set.drop()` crea una copia de `strat_train_set` sin la columna, en realidad no modifica `strat_train_set` en s√≠, a menos que pases `inplace=True`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpiar los datos (Data Cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mayor√≠a de los algoritmos de ***machine learning*** no pueden funcionar si faltan caracteristicas. Crearemos funciones que se ocupen de ello.\n",
    "* Notamos que a `total_bedrooms` le faltan valores\n",
    "Tenemos tres opciones:\n",
    "1. Deshacernos de los distritos correspondientes\n",
    "2. Deshacernos de todo el atributo\n",
    "3. Establecer alg√∫n valor para esos valores (cero, la media, la mediana, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se enumeran 3 opciones para manejar los valores NaN:\n",
    "\n",
    "```python\n",
    "housing.dropna(subset=[\"total_bedrooms\"], inplace=True)    # option 1\n",
    "\n",
    "housing.drop(\"total_bedrooms\", axis=1)       # option 2\n",
    "\n",
    "median = housing[\"total_bedrooms\"].median()  # option 3\n",
    "housing[\"total_bedrooms\"].fillna(median, inplace=True)\n",
    "```\n",
    "Para cada opci√≥n, crearemos una copia de `housing` y trabajaremos sobre esa copia para evitar romper `housing`. Tambi√©n mostraremos la salida de cada opci√≥n, pero filtrando las filas que originalmente conten√≠an un valor NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_rows_idx = housing.isnull().any(axis=1)\n",
    "housing.loc[null_rows_idx].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_option1 = housing.copy()\n",
    "\n",
    "housing_option1.dropna(subset=[\"total_bedrooms\"], inplace=True)  # option 1\n",
    "\n",
    "housing_option1.loc[null_rows_idx].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_option2 = housing.copy()\n",
    "\n",
    "housing_option2.drop(\"total_bedrooms\", axis=1, inplace=True)  # option 2\n",
    "\n",
    "housing_option2.loc[null_rows_idx].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_option3 = housing.copy()\n",
    "\n",
    "median = housing[\"total_bedrooms\"].median()\n",
    "housing_option3[\"total_bedrooms\"].fillna(median, inplace=True)  # option 3\n",
    "\n",
    "housing_option3.loc[null_rows_idx].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn ofrece una clase √∫til para ocuparse de los valores que faltan `SimpleImputer`:\n",
    "* Se debe crear una instancia `SimpleImputer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separar los atributos num√©ricos de los no n√∫mericos para utilizar la estrategia `\"median\"`. No se puede calcular sobre atributos de texto como `ocean_proximity`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num = housing.select_dtypes(include=[np.number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se pude ajustar la instancia `imputer` a los datos de entrenamiento utilizando el m√©todo `fit()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.fit(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La instancia `imputer` calcula la media de cada atributo y ha almacenado el resultado en su variable de instancia `statistics_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solo le faltaban valores al atributo `total_bedrooms`, pero no podemos estar seguros de que no faltar√°n valores en datos nuevos despu√©s de que se lance el sistema. Es m√°s seguro aplica `imputer` a todos los atributos n√∫mericos.\n",
    "Compruebe que es lo mismo que calcular manualmente la mediana de cada atributo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num.median().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformar el conjunto de entrenamiento utilizando la `imputer` entrenada para todo el datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputer.transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado es una matriz NumPy sencila que contiene las caracter√≠sticas transformadas.\n",
    "Si se quiere volver a ponerla en DataFrame de pandas es necesario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n",
    "                          index=housing_num.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede comprombar que la transformaci√≥n se realiz√≥ con √©xito:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr.loc[null_rows_idx].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminando los ***outliers***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolation Forest es un algoritmo de detecci√≥n de anomal√≠as basado en √°rboles de decisi√≥n aleatorios.\n",
    "- Utiliza una estrategia de particionamiento aleatorio para construir √°rboles de decisi√≥n de forma r√°pida y eficiente.\n",
    "- Para detectar una anomal√≠a, mide la profundidad del √°rbol necesario para aislarla de las observaciones normales.\n",
    "- Es un algoritmo r√°pido y escalable que puede manejar valores faltantes y datos categ√≥ricos en conjuntos de datos de alta dimensionalidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "isolation_forest = IsolationForest(random_state=42)\n",
    "outlier_pred = isolation_forest.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si desea eliminar los valores at√≠picos, ejecute el c√≥digo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#housing = housing.iloc[outlier_pred == 1]\n",
    "#housing_labels = housing_labels.iloc[outlier_pred == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamiento de atributos de texto y categ√≥ricos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora se han manejado atributos num√©ricos. Existen atributos de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a preprocesar la caracter√≠stica categ√≥rica de entrada, `ocean_proximity`.\n",
    "Revisemos las primeras 8 instancias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat = housing[[\"ocean_proximity\"]]\n",
    "housing_cat.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No es un texto arbitrario, hay un conjunto limitado de valores posibles de texto\n",
    "- Cada uno de los cuales representa una categor√≠a, por tanto es un atributo categ√≥rico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mayor√≠a de los algoritmos de ***machine learning*** prefiere trabajar con n√∫meros. As√≠ que se procede a convertir las catergor√≠as de texto en n√∫meros. Se utiliza la clase `OrdinalEncoder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat_encoded[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lista de categor√≠as procesadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Un problema de esta representaci√≥n es que los algoritmos de ***machine laerning*** asumen que dos valores ceranos son m√°s similares que dos valores distantes\n",
    "* Para solucionar este inconveniente, la mejor opci√≥n es crear un atributo binario ***one vs all***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La codificaci√≥n **one-hot** es un proceso mediante el cual se convierten variables categ√≥ricas en vectores binarios para que puedan ser utilizados como entradas en algoritmos de aprendizaje autom√°tico.\n",
    "* La codificaci√≥n **one-hot** asigna un valor binario de 1 a la categor√≠a presente y 0 para las dem√°s, creando un vector que representa la presencia o ausencia de cada categor√≠a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La salida de `OneHotEncoder` es una matriz dispersa de SciPy, en lugar de una matriz de NumPy.\n",
    "Para convertirla en una matriz densa, se utiliza el m√©todo `toarray()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativamente, puede establecer `sparse=False` al crear el `OneHotEncoder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder = OneHotEncoder(sparse=False)\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escalado de caracter√≠sticas\n",
    "\n",
    "Una dde las transformaciones m√°s importantes que hay que aplciar en los datos, salvo algunas excepeciones, el rendimiento no es bueno si no se hace un escalado previo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay dos formas de obtener la misma escala:\n",
    "1. Escalado min/max, llamado tambi√©n normalizaci√≥n: los valores se trasladan y escalan para quedar entre 0 y 1. Sustrae el valor m√≠nimo y divide entre el m√°ximo menos el m√≠nimo. Transformador: `MinMaxScaler`\n",
    "2. Normalizaci√≥n sustrae el valor medio (media cero) y luego se divide por la desviaci√≥n est√°ndar (resultado final con varianza unitaria). No hay rango de l√≠mites. Transformador: `StandardScaler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "housing_num_std_scaled = std_scaler.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines de transformaci√≥n\n",
    "\n",
    "Hay muchos pasaso de transformaci√≥nd edatos que hay que ejecutrar en el orden correcto.\n",
    "Scikit-Learn ofrece la clase `Pipeline` para ayudar con estas secuencias de transformaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a construir un pipeline sencillo para preprocesar los atributos num√©ricos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"standardize\", StandardScaler()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`make_pipeline` es una funci√≥n de Scikit-learn que permite crear un objeto de tuber√≠a (pipeline) para encadenar varios pasos de procesamiento de datos y un estimador final en un solo objeto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`set_config`  permite establecer opciones de configuraci√≥n globales para todos los estimadores y transformadores de la biblioteca:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "\n",
    "set_config(display='diagram')\n",
    "\n",
    "num_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente `fit_transform` es un m√©todo que se utiliza para ajustar y transformar los datos utilizando un objeto de tuber√≠a (pipeline) que encadena varios pasos de procesamiento de datos y un estimador final. Este m√©todo ajusta cada transformador en el pipeline a los datos de entrenamiento y transforma los datos de entrenamiento utilizando todos los transformadores en el orden especificado en el pipeline, y finalmente ajusta el estimador final a los datos transformados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num_prepared = num_pipeline.fit_transform(housing_num)\n",
    "housing_num_prepared[:2].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬°Todo bien! ¬°Es todo por hoy! üòÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
