{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marcoteran/deeplearning/blob/master/notebooks/1.2_machinelearning_imbalanceddata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Abrir en Colab\" title=\"Abrir y ejecutar en Google Colaboratory\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/marcoteran/deeplearning/blob/master/notebooks/1.2_machinelearning_imbalanceddata.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Abrir en Kaggle\" title=\"Abrir y ejecutar en Kaggle\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de código\n",
    "# Sesión 02: Manejo de clases desbalanceadas con la librería Python ImbLearn\n",
    "## Deep Learning y series de tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** Marco Teran\n",
    "**E-mail:** marco.teran@usa.edu.co\n",
    "\n",
    "[Website](http://marcoteran.github.io/),\n",
    "[Github](https://github.com/marcoteran),\n",
    "[LinkedIn](https://www.linkedin.com/in/marcoteran/).\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carcaterísticas del Dataset: Credit Card Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejercicio, se utilizará Python y el conjunto de datos [Credit Card Fraud Detection de Kaggle](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud):\n",
    "* Este conjunto de datos tiene un tamaño de 66 MB y se expande a 150 MB después de descomprimirlo\n",
    "* El archivo creditcard.csv consta de 285,000 filas y 31 columnas (características)\n",
    "* Excepto por las columnas Time y Amount (el importe de la transacción), la información de las características es privada y se nombran como V1, V2, V3, etc\n",
    "* Las clases están etiquetadas como 0 y 1, correspondiendo a \"transacción normal\" o \"fraude\"\n",
    "Debido a la desigualdad del conjunto de datos, hay muy pocas muestras etiquetadas como fraude. El enfoque se centra en aplicar diversas estrategias para mejorar los resultados, en lugar de elegir el modelo y ajustarlo en exceso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se dará prioridad a aplicar diversas estrategias para mejorar los resultados a pesar del desequilibrio de clases, en lugar de enfocarse en la elección y configuración del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requerimientos técnicos\n",
    "- Se necesitará Python 3.6 en el sistema, y se recomienda tener instalada la plataforma Anaconda para trabajar en una Notebook Jupyter\n",
    "- Es necesario instalar la librería **Imbalanced Learn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalación de la librería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U imbalanced-learn\n",
    "# conda install -c conda-forge imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:03:49.648431Z",
     "start_time": "2019-05-14T23:03:42.916007Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from pylab import rcParams\n",
    "\n",
    "from imblearn import under_sampling, over_sampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "#set up graphic style in this case I am using the color scheme from xkcd.com\n",
    "rcParams['figure.figsize'] = 14, 8.7 # Golden Mean\n",
    "LABELS = [\"Normal\",\"Fraud\"]\n",
    "#col_list = [\"cerulean\",\"scarlet\"]# https://xkcd.com/color/rgb/\n",
    "#sns.set(style='white', font_scale=1.75, palette=sns.xkcd_palette(col_list))\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis exploratorio de los datos\n",
    "\n",
    "Se debe realizar un análisis exploratorio para comprobar si hay desequilibrio entre las clases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descargar los datos\n",
    "Descarga de Kaggle en https://www.kaggle.com/mlg-ulb/creditcardfraud/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "def load_creditcard_data():\n",
    "    zip_file_path = Path(\"data/archive.zip\")\n",
    "    if not zip_file_path.is_file():\n",
    "        Path(\"data\").mkdir(parents=True, exist_ok=True)\n",
    "        url = \"https://github.com/marcoteran/deeplearning/raw/master/notebooks/data/archive.zip\"\n",
    "        urllib.request.urlretrieve(url, zip_file_path)\n",
    "    with zipfile.ZipFile(zip_file_path) as zip_file:\n",
    "        zip_file.extract(\"creditcard.csv\", path=\"data\")\n",
    "    return pd.read_csv(Path(\"data/creditcard.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de importar las librerías necesarias, se carga el dataframe con pandas y se observan las primeras filas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:03:55.243246Z",
     "start_time": "2019-05-14T23:03:49.651846Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = load_creditcard_data() # read in data downloaded to the local directory\n",
    "df.head(n=5) #just to check you imported the dataset properly4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante verificar la cantidad de filas del dataframe y la cantidad de filas de cada clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:03:55.257536Z",
     "start_time": "2019-05-14T23:03:55.246529Z"
    }
   },
   "outputs": [],
   "source": [
    "df.shape # Comprobación secundaria del tamaño del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:03:55.282798Z",
     "start_time": "2019-05-14T23:03:55.260837Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.value_counts(df['Class'], sort = True) #class comparison 0=Normal 1=Fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, el dataframe consta de 284,807 filas y sólo 492 filas pertenecen a la clase minoritaria de casos de fraude, lo que representa el 0.17% de las muestras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:03:55.493706Z",
     "start_time": "2019-05-14T23:03:55.286436Z"
    }
   },
   "outputs": [],
   "source": [
    "# Si no tienes una idea intuitiva de lo desequilibradas que están estas dos clases, vamos a lo visual\n",
    "count_classes = pd.value_counts(df['Class'], sort = True)\n",
    "count_classes.plot(kind = 'bar', rot=0)\n",
    "plt.xticks(range(2), LABELS)\n",
    "plt.title(\"Frecuencia por número de observación\")\n",
    "plt.xlabel(\"Clase\")\n",
    "plt.ylabel(\"Número de observaciones\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es necesario tener en cuenta que la línea roja que representa los casos de fraude es mínima debido a que son muy pocas muestras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:03:55.580502Z",
     "start_time": "2019-05-14T23:03:55.497973Z"
    }
   },
   "outputs": [],
   "source": [
    "normal_df = df[df.Class == 0] # guardar las observaciones de normal_df en un df separado\n",
    "fraud_df = df[df.Class == 1]  # hacer lo mismo con los fraudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:03:56.104227Z",
     "start_time": "2019-05-14T23:03:55.583067Z"
    }
   },
   "outputs": [],
   "source": [
    "# Trama de transacciones de alto valor\n",
    "bins = np.linspace(200, 2500, 100)\n",
    "plt.hist(normal_df.Amount, bins, alpha=1, normed=True, label='Normal')\n",
    "plt.hist(fraud_df.Amount, bins, alpha=0.6, normed=True, label='Fraud')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(\"Cantidad por porcentaje de transacciones (transacciones superiores a \\$200+)\")\n",
    "plt.xlabel(\"Cantidad de las transacciones (USD)\")\n",
    "plt.ylabel(\"Porcentaje de transacciones (%)\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:03:56.417389Z",
     "start_time": "2019-05-14T23:03:56.107770Z"
    }
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 48, 48) #48 hours\n",
    "plt.hist((normal_df.Time/(60*60)), bins, alpha=1, normed=True, label='Normal')\n",
    "plt.hist((fraud_df.Time/(60*60)), bins, alpha=0.6, normed=True, label='Fraud')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(\"Porcentaje de transacciones por hora\")\n",
    "plt.xlabel(\"Tiempo de transacción medido desde la primera transacción del conjunto de datos (horas)\")\n",
    "plt.ylabel(\"Porcentaje de transacciones (%)\");\n",
    "#plt.hist((df.Time/(60*60)),bins)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:03:58.767541Z",
     "start_time": "2019-05-14T23:03:56.421091Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter((normal_df.Time/(60*60)), normal_df.Amount, alpha=0.6, label='Normal')\n",
    "plt.scatter((fraud_df.Time/(60*60)), fraud_df.Amount, alpha=0.9, label='Fraud')\n",
    "plt.title(\"Cantidad de la transacción por hora\")\n",
    "plt.xlabel(\"Tiempo de transacción medido desde la primera transacción del conjunto de datos (horas)\")\n",
    "plt.ylabel(\"Cantidad (USD)\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos los datos de entrenamiento de las etiquetas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:03:58.950975Z",
     "start_time": "2019-05-14T23:03:58.771245Z"
    }
   },
   "outputs": [],
   "source": [
    "y = df['Class']\n",
    "X = df.drop('Class', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación reduciremos un conjunto de datos a 2 dimensiones para poder visualizarlos. Utilizaremos técnicas de reducción de dimensionalidad para lograr este objetivo y luego representaremos los datos en un gráfico de dispersión bidimensional. Esto nos permitirá tener una mejor comprensión de la estructura de los datos y de las relaciones entre las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T18:39:31.113587Z",
     "start_time": "2019-05-15T18:39:21.051870Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reducir el conjunto de datos a 2 dimensiones para visualizarlos\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "X_reduced = pca.transform(X)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize= (15,5))\n",
    "\n",
    "ax[0].scatter(X_reduced[y == 0, 0], X_reduced[y == 0, 1], label=\"Normal\", alpha=0.2)\n",
    "ax[0].scatter(X_reduced[y == 1, 0], X_reduced[y == 1, 1], label=\"Fraude\", alpha=0.2)\n",
    "ax[0].set_title('PCA del conjunto de datos original')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1] = sns.countplot(y)\n",
    "ax[1].set_title('Número de observaciones por clase')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del módelo\n",
    "Se representa una función llamada `run_model` que implementa un modelo de **regresión logística** utilizando la biblioteca Scikit-learn en Python.\n",
    "* La regresión logística es un tipo de algoritmo de aprendizaje supervisado que se utiliza comúnmente para la clasificación de datos.\n",
    "* El modelo utiliza una función ***sigmoide*** para asignar una probabilidad a cada clase y se entrena mediante la maximización de la función de verosimilitud.\n",
    "\n",
    "Se utilizan los siguientes parámetros para ajustar el modelo:\n",
    "- `C=1.0`, `penalty='l2'`, `random_state=1` y `solver=\"newton-cg\"`.\n",
    "\n",
    "El parámetro **C** controla la fuerza de regularización del modelo, mientras que **penalty** especifica el tipo de regularización a aplicar, y **solver** especifica el algoritmo de optimización utilizado para ajustar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:04:05.444946Z",
     "start_time": "2019-05-14T23:04:05.440299Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_model(X_train, X_test, y_train, y_test):\n",
    "    clf_base = LogisticRegression(C=1.0,penalty='l2',random_state=1,solver=\"newton-cg\")\n",
    "    clf_base.fit(X_train, y_train)\n",
    "    return clf_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo sin balancear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo aún sin balancear las clases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:04:20.885597Z",
     "start_time": "2019-05-14T23:04:05.448189Z"
    }
   },
   "outputs": [],
   "source": [
    "model = run_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se define una función que muestra la matriz de confusión y un informe de clasificación:\n",
    "- La matriz de confusión es una herramienta común para evaluar el rendimiento de un modelo de clasificación\n",
    "- El informe de clasificación proporciona métricas adicionales, como la precisión, la exhaustividad y la puntuación F1, para evaluar el rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:04:20.895907Z",
     "start_time": "2019-05-14T23:04:20.888843Z"
    }
   },
   "outputs": [],
   "source": [
    "def mostrar_resultados(y_test, pred_y):\n",
    "    conf_matrix = confusion_matrix(y_test, pred_y)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n",
    "    plt.title(\"Matriz de confusión\")\n",
    "    plt.ylabel('Clase verdadera')\n",
    "    plt.xlabel('Clase prevista')\n",
    "    plt.show()\n",
    "    print (classification_report(y_test, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:04:21.356620Z",
     "start_time": "2019-05-14T23:04:20.900093Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_y = model.predict(X_test)\n",
    "mostrar_resultados(y_test, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se presenta la matriz de confusión, y en particular se enfoca en la clase 2, que es la que se quiere detectar. Se observan 51 falsos negativos y 97 verdaderos positivos, lo que resulta en un recall de 0.66, un valor que se desea mejorar. Aunque los resultados de la columna de f1-score son buenos, no deben ser engañosos, ya que reflejan una realidad parcial. Es importante destacar que el modelo no es capaz de detectar correctamente los casos de fraude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estrategias para el tratamiento de datos desbalanceados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Estrategia: Penalización para compensar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el proceso de construcción del modelo de Regresión logística, se incorporará un parámetro adicional denominado `weight = balanced`. Esto permitirá que el algoritmo equilibre la clase minoritaria durante el entrenamiento.\n",
    "* Al establecer el parámetro **weight** en ***balanced***, el modelo asigna pesos diferentes a cada clase durante el entrenamiento para equilibrar la clase minoritaria\n",
    "* Se asignan pesos inversamente proporcionales a la frecuencia de las clases en los datos de entrenamiento.\n",
    "Esto significa que las observaciones de la clase minoritaria tendrán un **peso mayor** que las observaciones de la clase mayoritaria durante el entrenamiento, lo que permite que el modelo preste más atención a la clase minoritaria y reduzca el sesgo hacia la clase mayoritaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:04:42.322624Z",
     "start_time": "2019-05-14T23:04:21.359881Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_model_balanced(X_train, X_test, y_train, y_test):\n",
    "    clf = LogisticRegression(C=1.0,penalty='l2', random_state=1, solver=\"newton-cg\", class_weight=\"balanced\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corremos el modelo luego de aplicar la estrategia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = run_model_balanced(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validamos los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:04:42.757817Z",
     "start_time": "2019-05-14T23:04:42.325881Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_y = model.predict(X_test)\n",
    "mostrar_resultados(y_test, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al incorporar esta modificación, se ha obtenido una significativa mejora en la detección de casos fraudulentos, específicamente en la clase 2, que representa si hubo fraude. Se han acertado 137 muestras y se han fallado en 11, obteniendo un recall de 0.93. Se destaca que la inclusión de este parámetro no ha afectado negativamente el f1-score, sino que en realidad ha mejorado la capacidad del modelo para detectar casos de fraude. Es importante tener en cuenta que al aumentar los falsos positivos, se etiquetarán más muestras como fraudulentas cuando en realidad no lo son, pero esto es preferible a fallar en detectar los casos reales de fraude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Estrategia: Subsampling en la clase mayoritaria\n",
    "\n",
    "Por otro lado, se utilizará un algoritmo para reducir la clase mayoritaria. Este algoritmo se asemeja al **k-nearest neighbor** y permite seleccionar cuáles muestras eliminar. En este caso, se ha reducido significativamente de 199.020 a solo 688 muestras de la clase cero (la mayoría), con las cuales se entrenará el modelo.\n",
    "* NearMiss es un algoritmo de submuestreo utilizado para abordar el problema de desequilibrio de clases en los conjuntos de datos de entrenamiento.\n",
    "* Se basa en la idea de seleccionar las instancias de la clase mayoritaria que están más cercanas a las instancias de la clase minoritaria y eliminarlas para equilibrar las clases.\n",
    "* Hay tres versiones diferentes de NearMiss:\n",
    "    - **NearMiss-1** selecciona las instancias de la clase mayoritaria más cercanas al centro de la clase minoritaria\n",
    "    - **NearMiss-2** selecciona las instancias de la clase mayoritaria más lejanas a la clase minoritaria\n",
    "    - **NearMiss-3** selecciona las instancias de la clase mayoritaria que tienen la menor distancia media a sus k instancias más cercanas de la clase minoritaria.\n",
    "\n",
    "`sampling_strategy` se refiere a la proporción de instancias de la clase mayoritaria que se mantendrán después del submuestreo en relación con el número de instancias de la clase minoritaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:04:53.621130Z",
     "start_time": "2019-05-14T23:04:42.761449Z"
    }
   },
   "outputs": [],
   "source": [
    "us = NearMiss(sampling_strategy=0.5, n_neighbors=3, version=2)\n",
    "X_train_res, y_train_res = us.fit_sample(X_train, y_train)\n",
    "\n",
    "print (\"Distribución de las etiquetas de clase antes del resampling {}\".format(Counter(y_train)))\n",
    "print (\"Distribución de las etiquetas de clase despues del  resampling {}\".format(Counter(y_train_res)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corremos el modelo luego de aplicar la estrategia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:04:53.899048Z",
     "start_time": "2019-05-14T23:04:53.629703Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = run_model(X_train_res, X_test, y_train_res, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validamos los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:04:54.359016Z",
     "start_time": "2019-05-14T23:04:53.902615Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_y = model.predict(X_test)\n",
    "mostrar_resultados(y_test, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtiene un buen resultado con un recall de 0.93 aunque aumentaron los falsos positivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Estrategia: Random Oversampling de la clase minoritaria\n",
    "\n",
    "Se crean muestras sintéticas de la clase minoritaria utilizando `RandomOverSampler`. `RandomOverSampler` es una técnica de sobremuestreo para tratar el problema de desequilibrio de clases en los datos:\n",
    "* Duplica o triplica aleatoriamente las instancias de la clase minoritaria hasta que se equilibre con la clase mayoritaria.\n",
    "* Esto puede aumentar el riesgo de sobreajuste y generar datos sintéticos no realistas\n",
    "\n",
    "Los parámetros principales son:\n",
    "- \"sampling_strategy\", que determina la proporción deseada de instancias de la clase minoritaria\n",
    "- \"random_state\" para controlar la reproducibilidad\n",
    "- \"fit_resample\" para realizar el sobremuestreo y la adaptación del modelo en una sola llamada de función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:04:54.629840Z",
     "start_time": "2019-05-14T23:04:54.362403Z"
    }
   },
   "outputs": [],
   "source": [
    "os =  RandomOverSampler(sampling_strategy=0.5)\n",
    "X_train_res, y_train_res = os.fit_sample(X_train, y_train)\n",
    "\n",
    "print (\"Distribución de las etiquetas de clase antes del resampling {}\".format(Counter(y_train)))\n",
    "print (\"Distribución de las etiquetas de clase despues del  resampling {}\".format(Counter(y_train_res)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El número de muestras de fraude aumenta de 344 a 99.510."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se corre nuevamente el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:05:23.238880Z",
     "start_time": "2019-05-14T23:04:54.633097Z"
    }
   },
   "outputs": [],
   "source": [
    "model = run_model(X_train_res, X_test, y_train_res, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se validan los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:05:23.630293Z",
     "start_time": "2019-05-14T23:05:23.242376Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_y = model.predict(X_test)\n",
    "mostrar_resultados(y_test, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se alcanza un recall de 0.89 para la clase 2 y los falsos positivos son 838."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Estrategia: Combinamos resampling con Smote-Tomek\n",
    "\n",
    "Se utiliza la técnica de aplicar simultáneamente un algoritmo de subsampling y otro de oversampling al dataset. Se aplica SMOTE para oversampling y Tomek para undersampling. La técnica busca puntos vecinos cercanos y agrega puntos en línea recta entre ellos para oversampling, y quita los de distinta clase que sean nearest neighbor y deja ver mejor el decisión boundary para undersampling:\n",
    "* La submuestra de Tomek es una técnica de submuestreo que se utiliza para reducir el ruido y las instancias redundantes en los conjuntos de datos desequilibrados.\n",
    "* Tomek identifica las instancias que están muy cerca unas de otras, especialmente las de diferentes clases, y las elimina del conjunto de datos para mejorar la precisión del modelo.\n",
    "* Una característica importante de la implementación de SMOTE en la librería imbalanced-learn es que permite ajustar el número de vecinos cercanos que se utilizarán para la generación de nuevas instancias sintéticas, lo que puede influir en la calidad de las muestras generadas y, por lo tanto, en la efectividad del método."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicación de SMOTE y Tomek:\n",
    "- Sus parámetros son `sampling_strategy` para especificar la proporción deseada de la clase minoritaria, `random_state` para la reproducibilidad, y `n_jobs` para la paralelización del proceso de ajuste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:05:31.787106Z",
     "start_time": "2019-05-14T23:05:23.633252Z"
    }
   },
   "outputs": [],
   "source": [
    "os_us = SMOTETomek(sampling_strategy=0.5)\n",
    "X_train_res, y_train_res = os_us.fit_sample(X_train, y_train)\n",
    "\n",
    "print (\"Distribución de las etiquetas de clase antes del resampling {}\".format(Counter(y_train)))\n",
    "print (\"Distribución de las etiquetas de clase despues del  resampling {}\".format(Counter(y_train_res)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se corre nuevamente el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:06:00.657539Z",
     "start_time": "2019-05-14T23:05:31.790028Z"
    }
   },
   "outputs": [],
   "source": [
    "model = run_model(X_train_res, X_test, y_train_res, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se validan los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:06:01.062907Z",
     "start_time": "2019-05-14T23:06:00.661145Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_y = model.predict(X_test)\n",
    "mostrar_resultados(y_test, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtiene un buen recall de 0.85 para la clase 2 y se observa que hay pocos falsos positivos de la clase 1, 325 de 85,295 muestras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Estrategia: Ensamble de Modelos con Balanceo\n",
    "La estrategia de Ensamble de Modelos con Balanceo es una técnica que combina varios modelos de aprendizaje automático para mejorar la clasificación de conjuntos de datos desequilibrados.\n",
    "Algunas de sus características son:\n",
    "- El ensamblado permite combinar las fortalezas de varios modelos, lo que puede llevar a una mejora significativa en la precisión de la clasificación.\n",
    "- El balanceo permite corregir el sesgo inherente en los datos desequilibrados, lo que puede reducir los errores de clasificación y mejorar la capacidad de generalización del modelo.\n",
    "\n",
    "Los principales parámetros de configuración para la estrategia de ensamblado con balanceo incluyen:\n",
    "- Los modelos base que se utilizarán en el ensamblado, que pueden ser diferentes algoritmos de aprendizaje automático con diferentes parámetros.\n",
    "- El tipo de balanceo que se utilizará para corregir el desequilibrio en los datos, que puede incluir sobremuestreo, submuestreo o una combinación de ambos.\n",
    "- La estrategia de votación que se utilizará para combinar las predicciones de los modelos base, que puede ser ponderada o no ponderada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta estrategia se utilizará un Clasificador de Ensamble que emplea Bagging y un modelo de DecisionTree:\n",
    "* Bagging es una técnica de ensamblado de modelos que utiliza el muestreo de arranque para entrenar múltiples modelos independientes y combinar sus predicciones para obtener una predicción final más precisa.\n",
    "* Bagging ayuda a reducir la varianza y mejorar la precisión de los modelos, especialmente en conjuntos de datos de alta dimensionalidad y con ruido.\n",
    "* Los principales parámetros de configuración de Bagging son el tipo de modelo base, el número de modelos y la estrategia de votación utilizada para combinar las predicciones de los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:06:03.825482Z",
     "start_time": "2019-05-14T23:06:01.066317Z"
    }
   },
   "outputs": [],
   "source": [
    "# Crear un objeto del clasificador.\n",
    "bbc = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
    "                                sampling_strategy='auto',\n",
    "                                replacement=False,\n",
    "                                random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedemos a entrenar el clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento el clasificador.\n",
    "bbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testeo del clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = bbc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validación de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:06:04.389730Z",
     "start_time": "2019-05-14T23:06:03.828249Z"
    }
   },
   "outputs": [],
   "source": [
    "mostrar_resultados(y_test, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se evaluará su rendimiento y se observa una mejora en comparación con el modelo inicial, con un recall de 0.88 para los casos de fraude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparación de resultados "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto a los resultados obtenidos, se presentan en una tabla ordenada de mejor a peor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:10:42.373790Z",
     "start_time": "2019-05-14T23:10:42.356242Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'algorithm' : ['Regresion Logística', 'Penalizacion', 'NearMiss Subsampling', \n",
    "                                  'Random Oversampling', 'Smote Tomek', 'Ensemble'],\n",
    "                   'precision' : [1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "                   'recall' : [0.66, 0.93, 0.93, 0.89, 0.85, 0.88]})\n",
    "\n",
    "df['overall'] = df.apply(lambda row: (row.precision + row.recall)/2, axis=1)\n",
    "\n",
    "df = df.sort_values(['overall'], ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En este caso, las estrategias de Penalización y Subsampling obtienen el mejor resultado, con un recall de 0.93 cada una\n",
    "* Sin embargo, es importante destacar que todas las técnicas aplicadas logran mejorar el modelo inicial de Regresión Logística, que solo alcanzaba un 0.66 de recall para la clase de Fraude. Cabe recordar que el conjunto de datos presenta un desbalanceo considerable entre las clases.\n",
    "* Los modelos 1 y 2 (Penalización y NearMiss Subsampling) tienen el mayor valor de precision y recall, lo que indica que son capaces de clasificar correctamente tanto las instancias positivas como las negativas de la clase minoritaria.\n",
    "* El modelo 3 (Random Oversampling) tiene una precisión perfecta pero una tasa de recall ligeramente menor que los modelos 1 y 2.\n",
    "* El modelo 5 (Ensemble) tiene una precisión perfecta, pero su tasa de recall es menor que los modelos 1 y 2 y similar al modelo 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "¡Todo bien! ¡Es todo por hoy! 😀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
