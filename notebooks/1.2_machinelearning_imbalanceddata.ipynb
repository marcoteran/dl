{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marcoteran/deeplearning/blob/master/notebooks/1.2_machinelearning_imbalanceddata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Abrir en Colab\" title=\"Abrir y ejecutar en Google Colaboratory\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/marcoteran/deeplearning/blob/master/notebooks/1.2_machinelearning_imbalanceddata.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Abrir en Kaggle\" title=\"Abrir y ejecutar en Kaggle\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de c贸digo\n",
    "# Sesi贸n 02: Manejo de clases desbalanceadas con la librer铆a Python ImbLearn\n",
    "## Deep Learning y series de tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** Marco Teran\n",
    "**E-mail:** marco.teran@usa.edu.co\n",
    "\n",
    "[Website](http://marcoteran.github.io/),\n",
    "[Github](https://github.com/marcoteran),\n",
    "[LinkedIn](https://www.linkedin.com/in/marcoteran/).\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carcater铆sticas del Dataset: Credit Card Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejercicio, se utilizar谩 Python y el conjunto de datos [Credit Card Fraud Detection de Kaggle](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud):\n",
    "* Este conjunto de datos tiene un tama帽o de 66 MB y se expande a 150 MB despu茅s de descomprimirlo\n",
    "* El archivo creditcard.csv consta de 285,000 filas y 31 columnas (caracter铆sticas)\n",
    "* Excepto por las columnas Time y Amount (el importe de la transacci贸n), la informaci贸n de las caracter铆sticas es privada y se nombran como V1, V2, V3, etc\n",
    "* Las clases est谩n etiquetadas como 0 y 1, correspondiendo a \"transacci贸n normal\" o \"fraude\"\n",
    "Debido a la desigualdad del conjunto de datos, hay muy pocas muestras etiquetadas como fraude. El enfoque se centra en aplicar diversas estrategias para mejorar los resultados, en lugar de elegir el modelo y ajustarlo en exceso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se dar谩 prioridad a aplicar diversas estrategias para mejorar los resultados a pesar del desequilibrio de clases, en lugar de enfocarse en la elecci贸n y configuraci贸n del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requerimientos t茅cnicos\n",
    "- Se necesitar谩 Python 3.6 en el sistema, y se recomienda tener instalada la plataforma Anaconda para trabajar en una Notebook Jupyter\n",
    "- Es necesario instalar la librer铆a **Imbalanced Learn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalaci贸n de la librer铆a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U imbalanced-learn\n",
    "# conda install -c conda-forge imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:03:49.648431Z",
     "start_time": "2019-05-14T23:03:42.916007Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from pylab import rcParams\n",
    "\n",
    "from imblearn import under_sampling, over_sampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "#set up graphic style in this case I am using the color scheme from xkcd.com\n",
    "rcParams['figure.figsize'] = 14, 8.7 # Golden Mean\n",
    "LABELS = [\"Normal\",\"Fraud\"]\n",
    "#col_list = [\"cerulean\",\"scarlet\"]# https://xkcd.com/color/rgb/\n",
    "#sns.set(style='white', font_scale=1.75, palette=sns.xkcd_palette(col_list))\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis exploratorio de los datos\n",
    "\n",
    "Se debe realizar un an谩lisis exploratorio para comprobar si hay desequilibrio entre las clases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descargar los datos\n",
    "Descarga de Kaggle en https://www.kaggle.com/mlg-ulb/creditcardfraud/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "def load_creditcard_data():\n",
    "    zip_file_path = Path(\"data/archive.zip\")\n",
    "    if not zip_file_path.is_file():\n",
    "        Path(\"data\").mkdir(parents=True, exist_ok=True)\n",
    "        url = \"https://github.com/marcoteran/deeplearning/raw/master/notebooks/data/archive.zip\"\n",
    "        urllib.request.urlretrieve(url, zip_file_path)\n",
    "    with zipfile.ZipFile(zip_file_path) as zip_file:\n",
    "        zip_file.extract(\"creditcard.csv\", path=\"data\")\n",
    "    return pd.read_csv(Path(\"data/creditcard.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despu茅s de importar las librer铆as necesarias, se carga el dataframe con pandas y se observan las primeras filas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:03:55.243246Z",
     "start_time": "2019-05-14T23:03:49.651846Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = load_creditcard_data() # read in data downloaded to the local directory\n",
    "df.head(n=5) #just to check you imported the dataset properly4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante verificar la cantidad de filas del dataframe y la cantidad de filas de cada clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:03:55.257536Z",
     "start_time": "2019-05-14T23:03:55.246529Z"
    }
   },
   "outputs": [],
   "source": [
    "df.shape # Comprobaci贸n secundaria del tama帽o del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:03:55.282798Z",
     "start_time": "2019-05-14T23:03:55.260837Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.value_counts(df['Class'], sort = True) #class comparison 0=Normal 1=Fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, el dataframe consta de 284,807 filas y s贸lo 492 filas pertenecen a la clase minoritaria de casos de fraude, lo que representa el 0.17% de las muestras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:03:55.493706Z",
     "start_time": "2019-05-14T23:03:55.286436Z"
    }
   },
   "outputs": [],
   "source": [
    "# Si no tienes una idea intuitiva de lo desequilibradas que est谩n estas dos clases, vamos a lo visual\n",
    "count_classes = pd.value_counts(df['Class'], sort = True)\n",
    "count_classes.plot(kind = 'bar', rot=0)\n",
    "plt.xticks(range(2), LABELS)\n",
    "plt.title(\"Frecuencia por n煤mero de observaci贸n\")\n",
    "plt.xlabel(\"Clase\")\n",
    "plt.ylabel(\"N煤mero de observaciones\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es necesario tener en cuenta que la l铆nea roja que representa los casos de fraude es m铆nima debido a que son muy pocas muestras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:03:55.580502Z",
     "start_time": "2019-05-14T23:03:55.497973Z"
    }
   },
   "outputs": [],
   "source": [
    "normal_df = df[df.Class == 0] # guardar las observaciones de normal_df en un df separado\n",
    "fraud_df = df[df.Class == 1]  # hacer lo mismo con los fraudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:03:56.104227Z",
     "start_time": "2019-05-14T23:03:55.583067Z"
    }
   },
   "outputs": [],
   "source": [
    "# Trama de transacciones de alto valor\n",
    "bins = np.linspace(200, 2500, 100)\n",
    "plt.hist(normal_df.Amount, bins, alpha=1, normed=True, label='Normal')\n",
    "plt.hist(fraud_df.Amount, bins, alpha=0.6, normed=True, label='Fraud')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(\"Cantidad por porcentaje de transacciones (transacciones superiores a \\$200+)\")\n",
    "plt.xlabel(\"Cantidad de las transacciones (USD)\")\n",
    "plt.ylabel(\"Porcentaje de transacciones (%)\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:03:56.417389Z",
     "start_time": "2019-05-14T23:03:56.107770Z"
    }
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 48, 48) #48 hours\n",
    "plt.hist((normal_df.Time/(60*60)), bins, alpha=1, normed=True, label='Normal')\n",
    "plt.hist((fraud_df.Time/(60*60)), bins, alpha=0.6, normed=True, label='Fraud')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(\"Porcentaje de transacciones por hora\")\n",
    "plt.xlabel(\"Tiempo de transacci贸n medido desde la primera transacci贸n del conjunto de datos (horas)\")\n",
    "plt.ylabel(\"Porcentaje de transacciones (%)\");\n",
    "#plt.hist((df.Time/(60*60)),bins)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:03:58.767541Z",
     "start_time": "2019-05-14T23:03:56.421091Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter((normal_df.Time/(60*60)), normal_df.Amount, alpha=0.6, label='Normal')\n",
    "plt.scatter((fraud_df.Time/(60*60)), fraud_df.Amount, alpha=0.9, label='Fraud')\n",
    "plt.title(\"Cantidad de la transacci贸n por hora\")\n",
    "plt.xlabel(\"Tiempo de transacci贸n medido desde la primera transacci贸n del conjunto de datos (horas)\")\n",
    "plt.ylabel(\"Cantidad (USD)\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos los datos de entrenamiento de las etiquetas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:03:58.950975Z",
     "start_time": "2019-05-14T23:03:58.771245Z"
    }
   },
   "outputs": [],
   "source": [
    "y = df['Class']\n",
    "X = df.drop('Class', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuaci贸n reduciremos un conjunto de datos a 2 dimensiones para poder visualizarlos. Utilizaremos t茅cnicas de reducci贸n de dimensionalidad para lograr este objetivo y luego representaremos los datos en un gr谩fico de dispersi贸n bidimensional. Esto nos permitir谩 tener una mejor comprensi贸n de la estructura de los datos y de las relaciones entre las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T18:39:31.113587Z",
     "start_time": "2019-05-15T18:39:21.051870Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reducir el conjunto de datos a 2 dimensiones para visualizarlos\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "X_reduced = pca.transform(X)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize= (15,5))\n",
    "\n",
    "ax[0].scatter(X_reduced[y == 0, 0], X_reduced[y == 0, 1], label=\"Normal\", alpha=0.2)\n",
    "ax[0].scatter(X_reduced[y == 1, 0], X_reduced[y == 1, 1], label=\"Fraude\", alpha=0.2)\n",
    "ax[0].set_title('PCA del conjunto de datos original')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1] = sns.countplot(y)\n",
    "ax[1].set_title('N煤mero de observaciones por clase')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creaci贸n del m贸delo\n",
    "Se representa una funci贸n llamada `run_model` que implementa un modelo de **regresi贸n log铆stica** utilizando la biblioteca Scikit-learn en Python.\n",
    "* La regresi贸n log铆stica es un tipo de algoritmo de aprendizaje supervisado que se utiliza com煤nmente para la clasificaci贸n de datos.\n",
    "* El modelo utiliza una funci贸n ***sigmoide*** para asignar una probabilidad a cada clase y se entrena mediante la maximizaci贸n de la funci贸n de verosimilitud.\n",
    "\n",
    "Se utilizan los siguientes par谩metros para ajustar el modelo:\n",
    "- `C=1.0`, `penalty='l2'`, `random_state=1` y `solver=\"newton-cg\"`.\n",
    "\n",
    "El par谩metro **C** controla la fuerza de regularizaci贸n del modelo, mientras que **penalty** especifica el tipo de regularizaci贸n a aplicar, y **solver** especifica el algoritmo de optimizaci贸n utilizado para ajustar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:04:05.444946Z",
     "start_time": "2019-05-14T23:04:05.440299Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_model(X_train, X_test, y_train, y_test):\n",
    "    clf_base = LogisticRegression(C=1.0,penalty='l2',random_state=1,solver=\"newton-cg\")\n",
    "    clf_base.fit(X_train, y_train)\n",
    "    return clf_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo sin balancear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo a煤n sin balancear las clases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:04:20.885597Z",
     "start_time": "2019-05-14T23:04:05.448189Z"
    }
   },
   "outputs": [],
   "source": [
    "model = run_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuaci贸n, se define una funci贸n que muestra la matriz de confusi贸n y un informe de clasificaci贸n:\n",
    "- La matriz de confusi贸n es una herramienta com煤n para evaluar el rendimiento de un modelo de clasificaci贸n\n",
    "- El informe de clasificaci贸n proporciona m茅tricas adicionales, como la precisi贸n, la exhaustividad y la puntuaci贸n F1, para evaluar el rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:04:20.895907Z",
     "start_time": "2019-05-14T23:04:20.888843Z"
    }
   },
   "outputs": [],
   "source": [
    "def mostrar_resultados(y_test, pred_y):\n",
    "    conf_matrix = confusion_matrix(y_test, pred_y)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n",
    "    plt.title(\"Matriz de confusi贸n\")\n",
    "    plt.ylabel('Clase verdadera')\n",
    "    plt.xlabel('Clase prevista')\n",
    "    plt.show()\n",
    "    print (classification_report(y_test, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:04:21.356620Z",
     "start_time": "2019-05-14T23:04:20.900093Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_y = model.predict(X_test)\n",
    "mostrar_resultados(y_test, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se presenta la matriz de confusi贸n, y en particular se enfoca en la clase 2, que es la que se quiere detectar. Se observan 51 falsos negativos y 97 verdaderos positivos, lo que resulta en un recall de 0.66, un valor que se desea mejorar. Aunque los resultados de la columna de f1-score son buenos, no deben ser enga帽osos, ya que reflejan una realidad parcial. Es importante destacar que el modelo no es capaz de detectar correctamente los casos de fraude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estrategias para el tratamiento de datos desbalanceados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Estrategia: Penalizaci贸n para compensar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el proceso de construcci贸n del modelo de Regresi贸n log铆stica, se incorporar谩 un par谩metro adicional denominado `weight = balanced`. Esto permitir谩 que el algoritmo equilibre la clase minoritaria durante el entrenamiento.\n",
    "* Al establecer el par谩metro **weight** en ***balanced***, el modelo asigna pesos diferentes a cada clase durante el entrenamiento para equilibrar la clase minoritaria\n",
    "* Se asignan pesos inversamente proporcionales a la frecuencia de las clases en los datos de entrenamiento.\n",
    "Esto significa que las observaciones de la clase minoritaria tendr谩n un **peso mayor** que las observaciones de la clase mayoritaria durante el entrenamiento, lo que permite que el modelo preste m谩s atenci贸n a la clase minoritaria y reduzca el sesgo hacia la clase mayoritaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:04:42.322624Z",
     "start_time": "2019-05-14T23:04:21.359881Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_model_balanced(X_train, X_test, y_train, y_test):\n",
    "    clf = LogisticRegression(C=1.0,penalty='l2', random_state=1, solver=\"newton-cg\", class_weight=\"balanced\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corremos el modelo luego de aplicar la estrategia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = run_model_balanced(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validamos los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:04:42.757817Z",
     "start_time": "2019-05-14T23:04:42.325881Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_y = model.predict(X_test)\n",
    "mostrar_resultados(y_test, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al incorporar esta modificaci贸n, se ha obtenido una significativa mejora en la detecci贸n de casos fraudulentos, espec铆ficamente en la clase 2, que representa si hubo fraude. Se han acertado 137 muestras y se han fallado en 11, obteniendo un recall de 0.93. Se destaca que la inclusi贸n de este par谩metro no ha afectado negativamente el f1-score, sino que en realidad ha mejorado la capacidad del modelo para detectar casos de fraude. Es importante tener en cuenta que al aumentar los falsos positivos, se etiquetar谩n m谩s muestras como fraudulentas cuando en realidad no lo son, pero esto es preferible a fallar en detectar los casos reales de fraude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Estrategia: Subsampling en la clase mayoritaria\n",
    "\n",
    "Por otro lado, se utilizar谩 un algoritmo para reducir la clase mayoritaria. Este algoritmo se asemeja al **k-nearest neighbor** y permite seleccionar cu谩les muestras eliminar. En este caso, se ha reducido significativamente de 199.020 a solo 688 muestras de la clase cero (la mayor铆a), con las cuales se entrenar谩 el modelo.\n",
    "* NearMiss es un algoritmo de submuestreo utilizado para abordar el problema de desequilibrio de clases en los conjuntos de datos de entrenamiento.\n",
    "* Se basa en la idea de seleccionar las instancias de la clase mayoritaria que est谩n m谩s cercanas a las instancias de la clase minoritaria y eliminarlas para equilibrar las clases.\n",
    "* Hay tres versiones diferentes de NearMiss:\n",
    "    - **NearMiss-1** selecciona las instancias de la clase mayoritaria m谩s cercanas al centro de la clase minoritaria\n",
    "    - **NearMiss-2** selecciona las instancias de la clase mayoritaria m谩s lejanas a la clase minoritaria\n",
    "    - **NearMiss-3** selecciona las instancias de la clase mayoritaria que tienen la menor distancia media a sus k instancias m谩s cercanas de la clase minoritaria.\n",
    "\n",
    "`sampling_strategy` se refiere a la proporci贸n de instancias de la clase mayoritaria que se mantendr谩n despu茅s del submuestreo en relaci贸n con el n煤mero de instancias de la clase minoritaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:04:53.621130Z",
     "start_time": "2019-05-14T23:04:42.761449Z"
    }
   },
   "outputs": [],
   "source": [
    "us = NearMiss(sampling_strategy=0.5, n_neighbors=3, version=2)\n",
    "X_train_res, y_train_res = us.fit_sample(X_train, y_train)\n",
    "\n",
    "print (\"Distribuci贸n de las etiquetas de clase antes del resampling {}\".format(Counter(y_train)))\n",
    "print (\"Distribuci贸n de las etiquetas de clase despues del  resampling {}\".format(Counter(y_train_res)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corremos el modelo luego de aplicar la estrategia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:04:53.899048Z",
     "start_time": "2019-05-14T23:04:53.629703Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = run_model(X_train_res, X_test, y_train_res, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validamos los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:04:54.359016Z",
     "start_time": "2019-05-14T23:04:53.902615Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_y = model.predict(X_test)\n",
    "mostrar_resultados(y_test, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtiene un buen resultado con un recall de 0.93 aunque aumentaron los falsos positivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Estrategia: Random Oversampling de la clase minoritaria\n",
    "\n",
    "Se crean muestras sint茅ticas de la clase minoritaria utilizando `RandomOverSampler`. `RandomOverSampler` es una t茅cnica de sobremuestreo para tratar el problema de desequilibrio de clases en los datos:\n",
    "* Duplica o triplica aleatoriamente las instancias de la clase minoritaria hasta que se equilibre con la clase mayoritaria.\n",
    "* Esto puede aumentar el riesgo de sobreajuste y generar datos sint茅ticos no realistas\n",
    "\n",
    "Los par谩metros principales son:\n",
    "- \"sampling_strategy\", que determina la proporci贸n deseada de instancias de la clase minoritaria\n",
    "- \"random_state\" para controlar la reproducibilidad\n",
    "- \"fit_resample\" para realizar el sobremuestreo y la adaptaci贸n del modelo en una sola llamada de funci贸n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:04:54.629840Z",
     "start_time": "2019-05-14T23:04:54.362403Z"
    }
   },
   "outputs": [],
   "source": [
    "os =  RandomOverSampler(sampling_strategy=0.5)\n",
    "X_train_res, y_train_res = os.fit_sample(X_train, y_train)\n",
    "\n",
    "print (\"Distribuci贸n de las etiquetas de clase antes del resampling {}\".format(Counter(y_train)))\n",
    "print (\"Distribuci贸n de las etiquetas de clase despues del  resampling {}\".format(Counter(y_train_res)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El n煤mero de muestras de fraude aumenta de 344 a 99.510."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se corre nuevamente el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:05:23.238880Z",
     "start_time": "2019-05-14T23:04:54.633097Z"
    }
   },
   "outputs": [],
   "source": [
    "model = run_model(X_train_res, X_test, y_train_res, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se validan los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:05:23.630293Z",
     "start_time": "2019-05-14T23:05:23.242376Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_y = model.predict(X_test)\n",
    "mostrar_resultados(y_test, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se alcanza un recall de 0.89 para la clase 2 y los falsos positivos son 838."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Estrategia: Combinamos resampling con Smote-Tomek\n",
    "\n",
    "Se utiliza la t茅cnica de aplicar simult谩neamente un algoritmo de subsampling y otro de oversampling al dataset. Se aplica SMOTE para oversampling y Tomek para undersampling. La t茅cnica busca puntos vecinos cercanos y agrega puntos en l铆nea recta entre ellos para oversampling, y quita los de distinta clase que sean nearest neighbor y deja ver mejor el decisi贸n boundary para undersampling:\n",
    "* La submuestra de Tomek es una t茅cnica de submuestreo que se utiliza para reducir el ruido y las instancias redundantes en los conjuntos de datos desequilibrados.\n",
    "* Tomek identifica las instancias que est谩n muy cerca unas de otras, especialmente las de diferentes clases, y las elimina del conjunto de datos para mejorar la precisi贸n del modelo.\n",
    "* Una caracter铆stica importante de la implementaci贸n de SMOTE en la librer铆a imbalanced-learn es que permite ajustar el n煤mero de vecinos cercanos que se utilizar谩n para la generaci贸n de nuevas instancias sint茅ticas, lo que puede influir en la calidad de las muestras generadas y, por lo tanto, en la efectividad del m茅todo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicaci贸n de SMOTE y Tomek:\n",
    "- Sus par谩metros son `sampling_strategy` para especificar la proporci贸n deseada de la clase minoritaria, `random_state` para la reproducibilidad, y `n_jobs` para la paralelizaci贸n del proceso de ajuste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:05:31.787106Z",
     "start_time": "2019-05-14T23:05:23.633252Z"
    }
   },
   "outputs": [],
   "source": [
    "os_us = SMOTETomek(sampling_strategy=0.5)\n",
    "X_train_res, y_train_res = os_us.fit_sample(X_train, y_train)\n",
    "\n",
    "print (\"Distribuci贸n de las etiquetas de clase antes del resampling {}\".format(Counter(y_train)))\n",
    "print (\"Distribuci贸n de las etiquetas de clase despues del  resampling {}\".format(Counter(y_train_res)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se corre nuevamente el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:06:00.657539Z",
     "start_time": "2019-05-14T23:05:31.790028Z"
    }
   },
   "outputs": [],
   "source": [
    "model = run_model(X_train_res, X_test, y_train_res, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se validan los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:06:01.062907Z",
     "start_time": "2019-05-14T23:06:00.661145Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_y = model.predict(X_test)\n",
    "mostrar_resultados(y_test, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtiene un buen recall de 0.85 para la clase 2 y se observa que hay pocos falsos positivos de la clase 1, 325 de 85,295 muestras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Estrategia: Ensamble de Modelos con Balanceo\n",
    "La estrategia de Ensamble de Modelos con Balanceo es una t茅cnica que combina varios modelos de aprendizaje autom谩tico para mejorar la clasificaci贸n de conjuntos de datos desequilibrados.\n",
    "Algunas de sus caracter铆sticas son:\n",
    "- El ensamblado permite combinar las fortalezas de varios modelos, lo que puede llevar a una mejora significativa en la precisi贸n de la clasificaci贸n.\n",
    "- El balanceo permite corregir el sesgo inherente en los datos desequilibrados, lo que puede reducir los errores de clasificaci贸n y mejorar la capacidad de generalizaci贸n del modelo.\n",
    "\n",
    "Los principales par谩metros de configuraci贸n para la estrategia de ensamblado con balanceo incluyen:\n",
    "- Los modelos base que se utilizar谩n en el ensamblado, que pueden ser diferentes algoritmos de aprendizaje autom谩tico con diferentes par谩metros.\n",
    "- El tipo de balanceo que se utilizar谩 para corregir el desequilibrio en los datos, que puede incluir sobremuestreo, submuestreo o una combinaci贸n de ambos.\n",
    "- La estrategia de votaci贸n que se utilizar谩 para combinar las predicciones de los modelos base, que puede ser ponderada o no ponderada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta estrategia se utilizar谩 un Clasificador de Ensamble que emplea Bagging y un modelo de DecisionTree:\n",
    "* Bagging es una t茅cnica de ensamblado de modelos que utiliza el muestreo de arranque para entrenar m煤ltiples modelos independientes y combinar sus predicciones para obtener una predicci贸n final m谩s precisa.\n",
    "* Bagging ayuda a reducir la varianza y mejorar la precisi贸n de los modelos, especialmente en conjuntos de datos de alta dimensionalidad y con ruido.\n",
    "* Los principales par谩metros de configuraci贸n de Bagging son el tipo de modelo base, el n煤mero de modelos y la estrategia de votaci贸n utilizada para combinar las predicciones de los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:06:03.825482Z",
     "start_time": "2019-05-14T23:06:01.066317Z"
    }
   },
   "outputs": [],
   "source": [
    "# Crear un objeto del clasificador.\n",
    "bbc = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
    "                                sampling_strategy='auto',\n",
    "                                replacement=False,\n",
    "                                random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedemos a entrenar el clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento el clasificador.\n",
    "bbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testeo del clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = bbc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validaci贸n de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:06:04.389730Z",
     "start_time": "2019-05-14T23:06:03.828249Z"
    }
   },
   "outputs": [],
   "source": [
    "mostrar_resultados(y_test, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se evaluar谩 su rendimiento y se observa una mejora en comparaci贸n con el modelo inicial, con un recall de 0.88 para los casos de fraude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaci贸n de resultados "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto a los resultados obtenidos, se presentan en una tabla ordenada de mejor a peor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:10:42.373790Z",
     "start_time": "2019-05-14T23:10:42.356242Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'algorithm' : ['Regresion Log铆stica', 'Penalizacion', 'NearMiss Subsampling', \n",
    "                                  'Random Oversampling', 'Smote Tomek', 'Ensemble'],\n",
    "                   'precision' : [1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "                   'recall' : [0.66, 0.93, 0.93, 0.89, 0.85, 0.88]})\n",
    "\n",
    "df['overall'] = df.apply(lambda row: (row.precision + row.recall)/2, axis=1)\n",
    "\n",
    "df = df.sort_values(['overall'], ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En este caso, las estrategias de Penalizaci贸n y Subsampling obtienen el mejor resultado, con un recall de 0.93 cada una\n",
    "* Sin embargo, es importante destacar que todas las t茅cnicas aplicadas logran mejorar el modelo inicial de Regresi贸n Log铆stica, que solo alcanzaba un 0.66 de recall para la clase de Fraude. Cabe recordar que el conjunto de datos presenta un desbalanceo considerable entre las clases.\n",
    "* Los modelos 1 y 2 (Penalizaci贸n y NearMiss Subsampling) tienen el mayor valor de precision y recall, lo que indica que son capaces de clasificar correctamente tanto las instancias positivas como las negativas de la clase minoritaria.\n",
    "* El modelo 3 (Random Oversampling) tiene una precisi贸n perfecta pero una tasa de recall ligeramente menor que los modelos 1 y 2.\n",
    "* El modelo 5 (Ensemble) tiene una precisi贸n perfecta, pero su tasa de recall es menor que los modelos 1 y 2 y similar al modelo 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "隆Todo bien! 隆Es todo por hoy! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
