{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marcoteran/deeplearning/blob/master/notebooks/2.4_machinelearning_dimensionalityreduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Abrir en Colab\" title=\"Abrir y ejecutar en Google Colaboratory\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/marcoteran/deeplearning/blob/master/notebooks/2.4_machinelearning_dimensionalityreduction.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Abrir en Kaggle\" title=\"Abrir y ejecutar en Kaggle\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de c贸digo\n",
    "# Sesi贸n 06: Reducci贸n de la dimensionalidad y aprendizaje de la representaci贸n\n",
    "## Deep Learning y series de tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** Marco Teran **E-mail:** marco.tulio.teran@gmail.com,\n",
    "[Website](http://marcoteran.github.io/),\n",
    "[Github](https://github.com/marcoteran),\n",
    "[LinkedIn](https://www.linkedin.com/in/marcoteran/).\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importar librer铆as importantes\n",
    "\n",
    "Definimos primero unas librer铆as y funciones que vamos a usar a durante la sesi贸n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -rf data/\n",
    "#!rm -rf data.z*\n",
    "#!mkdir -p data/\n",
    "#!wget -O data.zip https://github.com/marcoteran/deeplearning/raw/master/notebooks/data.zip\n",
    "#!unzip data.zip\n",
    "#!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An谩lisis de Componentes Principales (PCA)\n",
    "\n",
    "El **An谩lisis de Componentes Principales (PCA)** es una t茅cnica de reducci贸n de dimensionalidad lineal que se utiliza para extraer informaci贸n de un *espacio de alta dimensionalidad* al proyectarlo en un subespacio de menor dimensi贸n.\n",
    "* PCA intenta preservar las partes esenciales que tienen mayor variaci贸n en los datos y eliminar las partes no esenciales con menos variaci贸n.\n",
    "* Las dimensiones son caracter铆sticas que representan los datos\n",
    "    - Ejemplo: en una imagen de 28 X 28 p铆xeles hay 784 elementos de imagen que son las dimensiones o caracter铆sticas que juntas representan esa imagen.\n",
    "* PCA es una t茅cnica de reducci贸n de dimensionalidad no supervisada, lo que significa que se pueden agrupar puntos de datos similares basados en la correlaci贸n entre caracter铆sticas sin ninguna supervisi贸n o etiquetas.\n",
    "\n",
    "<img src=\"https://github.com/marcoteran/deeplearning/raw/master/notebooks/figures/examplepca.png\" width=\"30%\">\n",
    "\n",
    "PCA es un procedimiento estad铆stico que utiliza una transformaci贸n ortogonal para convertir un conjunto de observaciones de variables posiblemente correlacionadas en un conjunto de valores de variables linealmente no correlacionadas llamadas componentes principales.\n",
    "\n",
    "**Nota:** Caracter铆sticas, Dimensiones y Variables se refieren a lo mismo. Se utilizan indistintamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicaciones del PCA\n",
    "* **La Visualizaci贸n de Datos:** El problema en el mundo actual es la gran cantidad de datos y las variables/caracter铆sticas que definen esos datos. Para resolver un problema en el que los datos son clave, se necesita una exploraci贸n exhaustiva de los datos como encontrar c贸mo est谩n correlacionadas las variables o entender la distribuci贸n de algunas variables. La visualizaci贸n puede ser un desaf铆o y casi imposible debido a la gran cantidad de variables o dimensiones en las que se distribuyen los datos. PCA puede proyectar los datos en una dimensi贸n m谩s baja, permiti茅ndote visualizar los datos en un espacio 2D o 3D con el ojo humano.\n",
    "* **Aceleraci贸n de un Algoritmo de Machine Learning (ML):** Dado que la idea principal de PCA es la reducci贸n de la dimensionalidad, se puede aprovechar para acelerar el tiempo de entrenamiento y prueba de un algoritmo de ML si los datos tienen muchas caracter铆sticas y el aprendizaje del algoritmo de ML es demasiado lento. A nivel abstracto, se toma un conjunto de datos con muchas caracter铆sticas y se simplifica seleccionando algunos componentes principales de las caracter铆sticas originales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consulta [A Tutorial on Principal Component Analysis](https://www.cs.princeton.edu/picasso/mats/PCA-Tutorial-Intuition_jp.pdf) para una decripci贸n intuitiva y detallada de PCA y SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuici贸n detr谩s del PCA\n",
    "\n",
    "Tenemos los siguientes datos 2D y nos gustar铆a encontrar una proyecci贸n en 1D que preserve la m谩xima cantidad de variabilidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X = np.dot(np.random.random(size=(2, 2)), np.random.normal(size=(2, 200))).T+10\n",
    "\n",
    "# Centra los datos en 0,0\n",
    "X=X-np.mean(X, axis=0)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La proyecci贸n de un vector en otro es la proyecci贸n ortogonal de un vector sobre otro en un espacio vectorial, que mide la magnitud de la componente de un vector en la direcci贸n de otro vector.\n",
    "\n",
    "Recuerda que la proyecci贸n de un vector $\\vec{x}$ en otro vector $\\vec{v}$ (consulta [aqu铆](https://matthew-brett.github.io/teaching/vector_projection.html)) viene dada por:\n",
    "\n",
    "$$c = \\frac{\\vec{v}\\times \\vec{x}}{||\\vec{v}||^2}$$\n",
    "\n",
    "\n",
    "$$proj_\\vec{v} \\vec{x} = \\vec{v} c$$\n",
    "\n",
    "\n",
    "donde $c$ es el tama帽o de la proyecci贸n de  $\\vec{x}$ sobre $\\vec{v}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspeccionamos algunas proyecciones\n",
    "\n",
    "El siguiente c贸digo de proyecci贸n de vectores representa la proyecci贸n de un conjunto de datos (X) en una direcci贸n aleatoria, usando la t茅cnica de proyecci贸n vectorial.\n",
    "* Se generan tres gr谩ficos que muestran la proyecci贸n de los datos en una direcci贸n aleatoria.\n",
    "* Para cada direcci贸n, se calcula el factor de escala de proyecci贸n (c) para ajustar la magnitud de la proyecci贸n.\n",
    "* Los puntos proyectados se muestran en rojo, mientras que los datos originales se muestran en azul.\n",
    "* La l铆nea negra representa el vector de proyecci贸n y la desviaci贸n est谩ndar de los factores de escala se muestra en el t铆tulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,3))\n",
    "\n",
    "unit_vector = lambda angle: np.array([np.cos(angle), np.sin(angle)])\n",
    "\n",
    "for i in range(3):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    angle = np.random.random()*np.pi*2\n",
    "    v = unit_vector(angle)\n",
    "\n",
    "    c = X.dot(v.reshape(-1,1))/(np.linalg.norm(v)**2)\n",
    "    Xp = np.repeat(v.reshape(-1,2),len(X),axis=0)*c\n",
    "\n",
    "    plt.scatter(X[:,0], X[:,1], color=\"blue\", alpha=.5, label=\"original data\")\n",
    "    plt.scatter(Xp[:,0], Xp[:,1], color=\"red\", alpha=.5, label=\"projected data\")\n",
    "    plt.axvline(0, color=\"gray\")\n",
    "    plt.axhline(0, color=\"gray\")\n",
    "    plt.plot([0,v[0]], [0,v[1]], color=\"black\", lw=3, label=\"projection vector\")\n",
    "    plt.axis('equal')\n",
    "    plt.ylim(-2,2)\n",
    "    plt.title(\"$\\\\alpha$=%.2f rads, proj std=%.3f\"%(angle, np.std(c)))\n",
    "    if i==2:\n",
    "        plt.legend(loc=\"center left\", bbox_to_anchor=(1.01,.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encontremos las proyecciones con mayor y menor std por fuerza bruta\n",
    "\n",
    "Se busca obtener las proyecciones de un conjunto de datos en diferentes 谩ngulos para encontrar la direcci贸n con la m谩xima y m铆nima variabilidad en los datos. Se muestra gr谩ficamente c贸mo var铆a la desviaci贸n est谩ndar de las proyecciones a medida que se rota el vector de proyecci贸n.\n",
    "* El c贸digo calcula la desviaci贸n est谩ndar de las proyecciones de un conjunto de datos en diferentes 谩ngulos.\n",
    "* Utiliza esta informaci贸n para encontrar la direcci贸n con la m谩xima y m铆nima variabilidad en los datos.\n",
    "* Muestra gr谩ficamente c贸mo var铆a la desviaci贸n est谩ndar de las proyecciones a medida que se rota el vector de proyecci贸n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maxmin_projections(X):\n",
    "    stds = []\n",
    "    angles = np.linspace(0,np.pi*2, 100)\n",
    "    for a in angles:\n",
    "        v = np.array([np.cos(a), np.sin(a)])\n",
    "        c = X.dot(v.reshape(-1,1))/(np.linalg.norm(v)**2)\n",
    "        stds.append(np.std(c))\n",
    "    v2 = unit_vector(angles[np.argmin(stds)])\n",
    "    v1 = unit_vector(angles[np.argmax(stds)])\n",
    "    \n",
    "    return angles, stds, v1, v2\n",
    "angles, stds, v1, v2 = get_maxmin_projections(X)\n",
    "\n",
    "plt.plot(angles, stds)\n",
    "plt.xlabel(\"projection $\\\\alpha$ (in rads)\")\n",
    "plt.ylabel(\"projection std\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], color=\"blue\", alpha=.5, label=\"original data\")\n",
    "plt.axvline(0, color=\"gray\")\n",
    "plt.axhline(0, color=\"gray\")\n",
    "plt.plot([0,v1[0]], [0,v1[1]], color=\"black\", lw=5, label=\"max std projection vector\")\n",
    "plt.plot([0,v2[0]], [0,v2[1]], color=\"black\", ls=\"--\", lw=2, label=\"min std projection vector\")\n",
    "plt.axis('equal')\n",
    "plt.ylim(-2,2)\n",
    "plt.legend(loc=\"center left\", bbox_to_anchor=(1.01,.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementaci贸n de PCA en Scikit-Learn\n",
    "\n",
    "PCA en Scikit-Learn se utiliza principalmente para reducir la dimensionalidad de los datos y permitir una visualizaci贸n m谩s f谩cil de los datos, pero tambi茅n puede ayudar a reducir el ruido y mejorar la precisi贸n del modelo de Machine Learning.\n",
    "* PCA en Scikit-Learn se puede utilizar en una variedad de aplicaciones, como la reducci贸n de dimensionalidad de datos, la visualizaci贸n de datos y la eliminaci贸n de caracter铆sticas irrelevantes o redundantes.\n",
    "* Scikit-Learn ofrece una implementaci贸n eficiente de PCA, que se puede ajustar a diferentes tipos de datos y configuraciones de par谩metros.\n",
    "* La funci贸n de PCA en Scikit-Learn tambi茅n proporciona una variedad de opciones para personalizar la salida y la interpretaci贸n de los resultados de PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Estos son los componentes principales**\n",
    "* **Observa que su dimensionalidad es la misma que los datos originales**\n",
    "\n",
    "Esto es lo que PCA nos da:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos el paquete PCA de scikit-learn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Creamos una instancia de PCA con dos componentes\n",
    "pca = PCA(n_components=2)\n",
    "# Ajustamos la instancia de PCA con los datos X\n",
    "pca.fit(X)\n",
    "\n",
    "# Imprimimos los componentes calculados por scikit-learn\n",
    "print(\"sklearn PCA components\")\n",
    "print(pca.components_)\n",
    "\n",
    "# Imprimimos los componentes calculados por fuerza bruta\n",
    "print(\"brute force components\")\n",
    "print(v1)\n",
    "print(v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero de modo mucho m谩s eficiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit get_maxmin_projections(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podemos usar el componente mayor para reducir la dimensionalidad de nuestros datos de 2D a 1D\n",
    "\n",
    "A continuaci贸n se muestra c贸mo podemos reducir la dimensionalidad de nuestros datos de 2D a 1D utilizando el componente mayor.\n",
    "* La ecuaci贸n que se muestra a continuaci贸n representa la transformaci贸n que se realiza en los datos originales para obtener los datos transformados:\n",
    "\n",
    "$$\\mathbf{X_t} = \\mathbf{X} \\times \\mathbf{V}$$\n",
    "\n",
    "Donde $\\mathbf{X}$ representa nuestros datos originales, $\\mathbf{V}$ es el vector de componentes seleccionados y $\\mathbf{X_t}$ son los datos transformados.\n",
    "\n",
    "Es importante mencionar que este tipo de transformaciones son lineales, es decir, que se realizan mediante rotaciones y escalados de los datos originales.\n",
    "* Esta restricci贸n nos permite utilizar t茅cnicas de 谩lgebra lineal para encontrar las transformaciones 贸ptimas que mejor representen nuestros datos (rotaciones y escalado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un objeto PCA con 1 componente para reducir la dimensionalidad de los datos de X a 1D\n",
    "pca = PCA(n_components=1)\n",
    "\n",
    "# Ajustamos el modelo PCA a los datos X\n",
    "pca.fit(X)\n",
    "\n",
    "# Transformamos los datos X al espacio reducido con PCA\n",
    "Xt = pca.transform(X)[:,0]\n",
    "\n",
    "# Graficamos los datos originales X en azul y los datos transformados Xt en rojo en el eje horizontal\n",
    "# El eje vertical est谩 fijado en 0 para visualizar mejor la reducci贸n de dimensionalidad\n",
    "plt.scatter(X[:,0], X[:,1], color=\"blue\", alpha=.5, label=\"$\\mathbf{X}$: original data\")\n",
    "plt.scatter(Xt, [0]*len(Xt), color=\"red\", alpha=.5, label=\"$\\mathbf{X_t}$: reduced data\")\n",
    "\n",
    "# Establecemos el mismo rango en ambos ejes para una mejor visualizaci贸n y agregamos una leyenda\n",
    "plt.axis(\"equal\");\n",
    "plt.legend(loc=\"center left\", bbox_to_anchor=(1.01,.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y podemos tambi茅n recontruir los datos 2D despu茅s de la transformaci贸n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos el primer componente principal\n",
    "v0 = pca.components_[0]\n",
    "\n",
    "# Proyectamos nuestros datos sobre el primer componente\n",
    "c = X.dot(v0)\n",
    "\n",
    "# Obtenemos la reconstrucci贸n de los datos proyectados usando 煤nicamente el primer componente\n",
    "Xr = np.r_[[i*v0 for i in c]]\n",
    "\n",
    "# Graficamos los datos originales y los datos reconstruidos\n",
    "plt.scatter(X[:,0], X[:,1], color=\"blue\", alpha=.5, label=\"original data\")\n",
    "plt.scatter(Xr[:,0], Xr[:,1], color=\"red\", alpha=.5, label=\"reconstructed data from largest component\")\n",
    "# Agregamos una leyenda y definimos el tama帽o de la figura\n",
    "plt.legend(loc=\"center left\", bbox_to_anchor=(1.01,.5))\n",
    "plt.figure(figsize=(8,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducci贸n de dimensionalidad para tareas de clasificaci贸n\n",
    "\n",
    "La reducci贸n de dimensionalidad PCA puede mejorar la eficiencia computacional de los modelos de clasificaci贸n al reducir el n煤mero de caracter铆sticas.\n",
    "* La eliminaci贸n de caracter铆sticas redundantes o irrelevantes mediante PCA puede mejorar la precisi贸n y generalizaci贸n del modelo.\n",
    "* PCA puede ayudar a visualizar mejor los datos y las relaciones entre las caracter铆sticas, lo que puede facilitar la interpretaci贸n y comprensi贸n del modelo y de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargando el dataset mnist1.5k con pandas\n",
    "mnist = pd.read_csv(\"data/mnist1.5k.csv.gz\", compression=\"gzip\", header=None).values\n",
    "\n",
    "# Seleccionando las columnas de im谩genes del dataset\n",
    "d = mnist[:, 1:785]\n",
    "\n",
    "# Seleccionando la columna de las clases del dataset\n",
    "c = mnist[:, 0]\n",
    "\n",
    "# Imprimiendo la dimensi贸n de las im谩genes y las clases\n",
    "print(\"Dimensi贸n de las im谩genes y las clases: \", d.shape, c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El c贸digo selecciona aleatoriamente 50 im谩genes y sus correspondientes etiquetas de un conjunto de datos de im谩genes y las muestra en una figura con sus etiquetas correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = np.random.permutation(range(d.shape[0]))[0:50]\n",
    "random_imgs   = d[perm]\n",
    "random_labels = c[perm] \n",
    "fig = plt.figure(figsize=(10,6))\n",
    "for i in range(random_imgs.shape[0]):\n",
    "    ax=fig.add_subplot(5,10,i+1)\n",
    "    plt.imshow(random_imgs[i].reshape(28,28), interpolation=\"nearest\", cmap = plt.cm.Greys_r)\n",
    "    ax.set_title(int(random_labels[i]))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis de componentes principales\n",
    "\n",
    "El siguiente c贸digo carga un conjunto de datos de im谩genes MNIST y realiza reducci贸n de dimensionalidad mediante el an谩lisis de componentes principales (PCA). Luego, muestra algunas de las componentes principales (PCs) obtenidas y tambi茅n visualiza algunas im谩genes originales y sus correspondientes reconstrucciones a partir de las PCs. Finalmente, utiliza un clasificador Naive Bayes y valida su desempe帽o en los datos originales y los datos transformados por PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lee el conjunto de datos de im谩genes MNIST y separa las caracter铆sticas (pixeles) de las etiquetas (n煤meros escritos a mano).\n",
    "mnist = pd.read_csv(\"data/mnist1.5k.csv.gz\", compression=\"gzip\", header=None).values\n",
    "X=mnist[:,1:785]\n",
    "y=mnist[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea una instancia de PCA con 60 componentes y ajusta los datos de entrenamiento.\n",
    "pca = PCA(n_components=60)\n",
    "Xp = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtenemos los componentes principales\n",
    "Visualiza 60 componentes principales como im谩genes de 28x28 p铆xeles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=20\n",
    "plt.figure(figsize=(15,3))\n",
    "for i in range(len(pca.components_)):\n",
    "    plt.subplot(np.ceil(len(pca.components_)/15.),15,i+1)\n",
    "    plt.imshow((pca.components_[i].reshape(28,28)), cmap = plt.cm.Greys_r)\n",
    "    plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verificamos la reconstrucci贸n con los componentes principales\n",
    "Visualiza 6 im谩genes originales y sus reconstrucciones a partir de las componentes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "for i in range(6):\n",
    "    plt.subplot(3,6,i+1)\n",
    "    k = np.random.randint(len(X))\n",
    "    plt.imshow((np.sum((pca.components_*Xp[k].reshape(-1,1)), axis=0)).reshape(28,28), cmap=plt.cm.Greys_r)\n",
    "    plt.xticks([]); plt.yticks([])\n",
    "    plt.subplot(3,6,6+i+1)\n",
    "    plt.imshow(X[k].reshape(28,28), cmap=plt.cm.Greys_r)\n",
    "    plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificaci贸n en el nuevo espacio de representaci贸n\n",
    "\n",
    "Utiliza el clasificador Naive Bayes y calcula el desempe帽o de la clasificaci贸n en los datos originales y los datos transformados por PCA.\n",
    "* El **clasificador de Naive Bayes** es un algoritmo de aprendizaje supervisado que se basa en el teorema de Bayes para predecir la clase de un dato nuevo a partir de sus caracter铆sticas. El teorema de Bayes es un principio matem谩tico que describe c贸mo actualizar la probabilidad de una hip贸tesis a medida que se obtiene nueva evidencia. Algunas caracter铆sticas importantes de este clasificador son:\n",
    "    - Es un m茅todo simple y r谩pido que puede ser utilizado en conjuntos de datos grandes.\n",
    "    - Es especialmente 煤til cuando se tienen muchas caracter铆sticas y se desea evitar el problema de la maldici贸n de la dimensionalidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "print(np.mean(cross_val_score(GaussianNB(), X, y, cv=5)))\n",
    "print(np.mean(cross_val_score(GaussianNB(), Xp, y, cv=5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El segundo modelo, el que utiliz贸 PCA, tiene un mejor rendimiento en t茅rminos de precisi贸n que el primero que no utiliz贸 PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observa la nueva representaci贸n de la primera imagen\n",
    "Muestra la primera instancia de los datos transformados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xp[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando pipelines\n",
    "\n",
    "Debemos de tener cuidado cuando usamos transformaciones en clasificaci贸n, ya que tenemos que ajustarlas (de manera no supervisada) s贸lo con los datos de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la clase Pipeline del m贸dulo sklearn.pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Creamos un pipeline que primero hace reducci贸n de dimensionalidad PCA y luego aplica el clasificador Naive Bayes\n",
    "pip = Pipeline([(\"PCA\", PCA(n_components=60)), (\"gaussian\", GaussianNB())])\n",
    "\n",
    "# Obtenemos el resultado del score de validaci贸n cruzada utilizando el pipeline en los datos originales X e y\n",
    "print(np.mean(cross_val_score(pip, X,y, cv=5 )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition\n",
    "\n",
    "Singular Value Decomposition (SVD) es una t茅cnica matem谩tica que descompone una matriz de datos en tres matrices m谩s simples y significativas que pueden utilizarse para reducir la dimensionalidad de los datos, extraer patrones y caracter铆sticas, y hacer la recomposici贸n de los datos originales.\n",
    "\n",
    "Caracter铆sticas de SVD:\n",
    "* Puede ser utilizado para reducci贸n de dimensionalidad, detecci贸n de patrones y caracter铆sticas, y an谩lisis de correlaci贸n.\n",
    "* Es una t茅cnica no supervisada que no requiere de etiquetas o clases de datos para operar.\n",
    "* Es ampliamente utilizado en aplicaciones como procesamiento de se帽ales, reconocimiento de voz, miner铆a de datos y aprendizaje autom谩tico.\n",
    "\n",
    "En la f贸rmula presentada, $\\mathbf{X}$ representa la matriz de datos original que se desea descomponer en tres matrices m谩s simples. $\\mathbf{U}$ y $\\mathbf{V}^*$ representan matrices unitarias que contienen informaci贸n sobre las direcciones y magnitudes de las variaciones de los datos en cada dimensi贸n. \n",
    "\n",
    "$$\\mathbf{X} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^*$$ \n",
    "\n",
    "donde:\n",
    "\n",
    "- $\\mathbf{X}$ son nuestros datos\n",
    "- $\\mathbf{U}$ es unitaria (sus columnas y filas son ortonormales, forman una base)\n",
    "- $\\mathbf{V}^*$ es unitaria (sus columnas y filas son ortonormales, forman una base)\n",
    "\n",
    "\n",
    "$\\mathbf{\\Sigma}$ es una matriz diagonal que contiene los valores singulares, que representan la importancia relativa de cada dimensi贸n.\n",
    "La matriz $\\mathbf{X}$ puede ser recompuesta como el producto de las tres matrices: $\\mathbf{U}$, $\\mathbf{\\Sigma}$ y $\\mathbf{V}^*$, lo que permite una representaci贸n m谩s simple y eficiente de los datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la descomposici贸n en valores singulares de los datos de entrada X\n",
    "U, s, V = np.linalg.svd(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la forma de las matrices U, s y V\n",
    "# U es una matriz unitaria cuyas columnas y filas son ortonormales (forman una base)\n",
    "# s es un vector que contiene los valores singulares de X\n",
    "# V es una matriz unitaria cuyas columnas y filas son ortonormales (forman una base)\n",
    "U.shape, s.shape, V.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruimos la matriz diagonal s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir el vector de valores singulares en una matriz diagonal\n",
    "s = np.diag(s)\n",
    "# A帽adir filas de ceros a la matriz de valores singulares para que tenga la misma forma que U\n",
    "s = np.vstack([s, np.zeros((U.shape[0]-s.shape[0], s.shape[1]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos las propiedades SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"verificando si U es unitaria: \", np.allclose(U.dot(U.T), np.eye(U.shape[0])))\n",
    "print(\"verificando si las filas de U son unitarias: \", np.allclose(np.linalg.norm(U, axis=1), np.ones(U.shape[0])))\n",
    "print(\"verificando si las columnas de U son unitarias: \", np.allclose(np.linalg.norm(U, axis=0), np.ones(U.shape[1])))\n",
    "print(\"verificando si V es unitaria: \", np.allclose(V.T.dot(V), np.eye(V.shape[0])))\n",
    "print(\"verificando si las filas de V son unitarias: \", np.allclose(np.linalg.norm(V, axis=1), np.ones(V.shape[0])))\n",
    "print(\"verificando si las columnas de V son unitarias: \", np.allclose(np.linalg.norm(V, axis=0), np.ones(V.shape[1])))\n",
    "print(\"verificando la reconstrucci贸n de X: \", np.allclose(U.dot(s).dot(V), X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Sigma$ viene ordenada, y cada coeficiente cuantifica cuanto contribuye cada base en $V$ a la variabilidad de los datos originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.diagonal(s))\n",
    "plt.xlabel(\"component index of $\\Sigma$\");\n",
    "plt.ylabel(\"component value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observa que los componentes de PCA y $V^*$ de SVD son los mismos\n",
    "\n",
    "Aunque a veces vengan con signo distinto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xm = X-np.mean(X, axis=0)\n",
    "U,s,V = np.linalg.svd(Xm)\n",
    "pca = PCA(n_components=60)\n",
    "pca.fit(Xm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(pca.n_components)\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.subplot(121)\n",
    "plt.plot(pca.components_[i])\n",
    "plt.title(\"PCA component %d\"%i)\n",
    "plt.subplot(122)\n",
    "plt.title(\"SVD $V^*$ row %d\"%i)\n",
    "plt.plot(V[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo si queremos preservar solamente los vectores base de SVD que contienen el 40% de la variabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = np.argwhere(np.cumsum(s)/np.sum(s)>.4)[0][0]\n",
    "print(\"Keeping %d components\"%n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = V[:n_components]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=20\n",
    "plt.figure(figsize=(15,3))\n",
    "for i in range(len(c)):\n",
    "    plt.subplot(np.ceil(len(c)/15.),15,i+1)\n",
    "    plt.imshow((c[i].reshape(28,28)), cmap = plt.cm.Greys_r)\n",
    "    plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo cual corresponde a los mismos componentes mencionados arriba para PCA.\n",
    "\n",
    "Los componentes de PCA y las columnas de la matriz $V^*$ de SVD son los mismos porque el proceso de SVD es esencialmente un proceso de PCA. Al aplicar SVD a una matriz de datos, la matriz de $V^*$ devuelve las componentes principales de la matriz original. Por lo tanto, los vectores de la matriz de $V^*$ se pueden utilizar como componentes principales para la reducci贸n de la dimensionalidad, de la misma manera que los componentes principales se obtienen directamente del proceso de PCA.\n",
    "\n",
    "Ambos m茅todos realizan una descomposici贸n de los datos para identificar las componentes principales y reducir la dimensionalidad, por lo que los componentes resultantes son los mismos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non negative matrix factorization\n",
    "\n",
    "Non-negative matrix factorization (NMF) es un m茅todo de factorizaci贸n de matrices que descompone una matriz no negativa en dos matrices no negativas de menor rango.\n",
    "\n",
    "Caracter铆sticas de NMF:\n",
    "* Se utiliza para reducci贸n de dimensionalidad, extracci贸n de caracter铆sticas y clustering.\n",
    "* Las matrices resultantes son interpretables y pueden ser utilizadas para identificar patrones y relaciones entre los datos.\n",
    "* Es especialmente 煤til para datos en los que se espera que los valores sean no negativos, como en im谩genes o se帽ales.\n",
    "\n",
    "El objetivo de NMF es descomponer una matriz no negativa $V \\in \\mathbb{R}+^{m\\times n}$ en el producto $W \\times H$, donde $W \\in \\mathbb{R}+^{m\\times r}$ y $H \\in \\mathbb{R}+^{r\\times n}$ con la restricci贸n de que todo sea positivo ($\\in \\mathbb{R}+$). Esto se hace de forma que la aproximaci贸n\n",
    "\n",
    "$$V \\approx W \\times H$$\n",
    "\n",
    "tenga un error m铆nimo.\n",
    "\n",
    "Las filas de $H$ son los _componentes base_ que se utilizan para reconstruir la matriz original., y se soluciona plante谩ndolo como un problema de optimizaci贸n matem谩tica con restricciones.\n",
    "\n",
    "$$\\begin{split}\n",
    "argmin_{W,H}\\;& ||V-W\\times H||\\\\\n",
    "s.t.&\\;W,H \\in \\mathbb{R}_+\n",
    "\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/marcoteran/deeplearning/raw/master/notebooks/figures/nmf.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtenemos la descomposici贸n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar la funci贸n NMF del m贸dulo sklearn.decomposition\n",
    "from sklearn.decomposition import NMF\n",
    "# Cargar los datos de la base de datos mnist\n",
    "X = mnist[:,1:785]; y = mnist[:,0]\n",
    "\n",
    "#Definir un modelo NMF con 15 componentes y una inicializaci贸n aleatoria\n",
    "nmf = NMF(n_components=15, init=\"random\")\n",
    "\n",
    "#Ajustar el modelo y transformar los datos\n",
    "Xn = nmf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el n煤mero de columnas para la figura\n",
    "cols = 20\n",
    "#Crear una figura de tama帽o 15x3\n",
    "plt.figure(figsize=(15,3))\n",
    "\n",
    "#Recorrer cada componente del modelo NMF\n",
    "for i in range(len(nmf.components_)):\n",
    "    plt.subplot(len(nmf.components_)/15,15,i+1)# Crear un subplot para cada componente\n",
    "    # Mostrar el componente como una imagen de escala de grises\n",
    "    plt.imshow(np.abs(nmf.components_[i].reshape(28,28)), cmap = plt.cm.Greys_r)\n",
    "    plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar la primera fila de los datos transformados\n",
    "Xn[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificamos la reconstrucci贸n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una figura de tama帽o 10x6\n",
    "plt.figure(figsize=(10,6))\n",
    "for i in range(6):\n",
    "    plt.subplot(3,6,i+1)\n",
    "    # Escoger un 铆ndice de ejemplo aleatorio\n",
    "    k = np.random.randint(len(X))\n",
    "    plt.imshow(np.abs(np.sum((nmf.components_*Xn[k].reshape(-1,1)), axis=0)).reshape(28,28), cmap=plt.cm.Greys_r)\n",
    "    plt.xticks([]); plt.yticks([])\n",
    "    plt.subplot(3,6,6+i+1)\n",
    "    plt.imshow(X[k].reshape(28,28), cmap=plt.cm.Greys_r)\n",
    "    plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificamos en el nuevo espacio de representaci贸n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar la media de la puntuaci贸n de la validaci贸n cruzada con un modelo GaussianNB en los datos originales\n",
    "print(np.mean(cross_val_score(GaussianNB(), X,y, cv=5)))\n",
    "# Mostrar la media de la puntuaci贸n de la validaci贸n cruzada con un modelo GaussianNB en los datos transformados\n",
    "print(np.mean(cross_val_score(GaussianNB(), Xn,y, cv=5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La primera imagen en el nuevo espacio de representaci贸n\n",
    "Observa que todos los componentes son positivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar la primera fila de los datos transformados\n",
    "Xn[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: NMF para el reconocimiento de rostros\n",
    "\n",
    "El c贸digo carga un conjunto de datos de im谩genes de caras, muestra algunas de ellas en una figura y luego aplica Non-negative Matrix Factorization (NMF) para reducir la dimensionalidad de las im谩genes y extraer caracter铆sticas relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se importa la biblioteca pickle para cargar datos almacenados en formato pickle\n",
    "import pickle\n",
    "faces = pickle.load(open(\"data/faces.pkl\", \"rb\"), encoding='latin1')\n",
    "faces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea una figura con una dimension de 15x2\n",
    "plt.figure(figsize=(15,2))\n",
    "for i in range(30):\n",
    "    plt.subplot(2,15,i+1)\n",
    "    plt.imshow(faces[np.random.randint(len(faces))].reshape(19,19), cmap=plt.cm.Greys_r)\n",
    "    plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecci贸n aleatoria de 30 caras de tama帽o 19x19. Se utiliza la biblioteca scikit-learn para aplicar NMF en dos versiones diferentes (con diferentes opciones de inicializaci贸n) y visualiza las caracter铆sticas extra铆das en dos figuras diferentes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea un objeto NMF con 30 componentes y se inicializa aleatoriamente\n",
    "nmf      = NMF(n_components=30, init=\"random\")\n",
    "# Se aplica la reducci贸n de dimensiones al conjunto de datos 'faces'\n",
    "faces_n  = nmf.fit_transform(faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultados\n",
    "cols=20\n",
    "plt.figure(figsize=(15,2))\n",
    "# Se imprime la suma de las componentes de NMF\n",
    "print(np.sum(nmf.components_))\n",
    "for i in range(len(nmf.components_)):\n",
    "    plt.subplot(np.ceil(len(nmf.components_)/15.),15,i+1)\n",
    "    plt.imshow(np.abs(nmf.components_[i].reshape(19,19)), cmap = plt.cm.Greys)\n",
    "    plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ecuaci贸n que se muestra a continuaci贸n es una forma de la factorizaci贸n de matrices no negativas (NMF) con una restricci贸n de dispersi贸n en los componentes.\n",
    "* La NMF es una t茅cnica de reducci贸n de dimensiones en la que se descompone una matriz de datos en dos matrices m谩s peque帽as, una matriz de componentes base y una matriz de pesos, que combinados pueden aproximarse a la matriz original.\n",
    "* En esta formulaci贸n, se utiliza la norma $L_1$ en los componentes base para forzar la dispersi贸n, lo que significa que los componentes base tendr谩n valores peque帽os y dispersos en lugar de valores grandes y densamente distribuidos.\n",
    "* La restricci贸n $W,H \\in \\mathbb{R}_+$ significa que todas las entradas de las matrices $W$ y $H$ deben ser no negativas.\n",
    "\n",
    "Forzamos dispersi贸n en los componentes, y extendemos el problema de optimizaci贸n con la norma $L_1$ en los componentes base.\n",
    "\n",
    "$$\\begin{split}\n",
    "argmin_{W,H}\\;& ||V-W\\times H|| + ||H||^2_1\\\\\n",
    "s.t.&\\;W,H \\in \\mathbb{R}_+\n",
    "\\end{split}$$\n",
    "\n",
    "Una forma alternativa de la NMF que fuerza la dispersi贸n en la nueva representaci贸n, utilizando la norma $L_1$ en la matriz $W$\n",
    "$$\\begin{split}\n",
    "argmin_{W,H}\\;& ||V-W\\times H|| + ||W||^2_1\\\\\n",
    "s.t.&\\;W,H \\in \\mathbb{R}_+\n",
    "\\end{split}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea un objeto NMF con 30 componentes, inicializadas por una descomposici贸n en valores singulares no negativos y con una relaci贸n L1 de 1\n",
    "nmf      = NMF(n_components=30, init=\"nndsvd\", l1_ratio=1)\n",
    "# Se aplica la reducci贸n de dimensiones al conjunto de datos 'faces'\n",
    "faces_n  = nmf.fit_transform(faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultados\n",
    "cols=20\n",
    "plt.figure(figsize=(15,2))\n",
    "# Se imprime la suma de las componentes de NMF\n",
    "print(np.sum(nmf.components_))\n",
    "for i in range(len(nmf.components_)):\n",
    "    plt.subplot(np.ceil(len(nmf.components_)/15.),15,i+1)\n",
    "    plt.imshow(np.abs(nmf.components_[i].reshape(19,19)), cmap = plt.cm.Greys)\n",
    "    plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "隆Todo bien! 隆Es todo por hoy! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taller\n",
    "\n",
    "### Ejercicio 1\n",
    "\n",
    "* Cargamos el conjunto de datos de movimientos en la bolsa que est谩 en la ubicaci贸n `data/company-stock-movements-2010-2015-incl.csv.gz`\n",
    "* Convertimos todos los valores en 1 si > 0 y -1 en otro caso\n",
    "* Aplicamos PCA con 2 componentes al conjunto de datos reci茅n modificado (con +1/-1)\n",
    "* Usa KMeans con 7 clusters\n",
    "* Visualiza los clusters de KMeans en el plano 2D dado por PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 7\n",
    "\n",
    "X = PCA ... <YOUR CODE HERE>\n",
    "y = KMeans ... <YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifica tu c贸digo. Debe de aparecer aproximadamente como esto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"imgs/companies_clustering.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C贸digo de ayuda para imprimir un texto al lado de cada punto en el plano 2D**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.hot\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(X[:,0], X[:,1], color=cmap((y*255./(n_clusters-1)).astype(int)), s=100, edgecolor=\"black\", lw=2)\n",
    "for i in range(len(d)):\n",
    "    name = d.index[i]\n",
    "    plt.text(X[i,0]+.1, X[i,1]+.1, d.index[i], fontsize=14)\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EJercicio 3: Bag of features\n",
    "Construiremos en este conjunto de problemas una implementaci贸n del m茅todo de _Bag of features_, sobre el dataset MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='imgs/bof.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecuta las siguientes celdas para cargar MNIST y ver una muestra del mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = pd.read_csv(\"data/mnist1.5k.csv.gz\", compression=\"gzip\", header=None).values.astype(float)\n",
    "print(\"Dimensi贸n de los datos originales\", mnist.shape)\n",
    "X=mnist[:,1:785]\n",
    "y=mnist[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = np.random.permutation(range(X.shape[0]))[0:50]\n",
    "random_imgs   = X[perm]\n",
    "random_labels = y[perm] \n",
    "fig = plt.figure(figsize=(10,6))\n",
    "for i in range(random_imgs.shape[0]):\n",
    "    ax=fig.add_subplot(5,10,i+1)\n",
    "    plt.imshow(random_imgs[i].reshape(28,28), interpolation=\"nearest\", cmap = plt.cm.Greys_r)\n",
    "    ax.set_title(int(random_labels[i]))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 4: Extracci贸n de parches\n",
    "\n",
    "Completa la siguiente funci贸n para que dada una imagen en escala de grises ($\\in [0,255]^{h\\times w}$), extraiga parches de tama帽o cuadrado de la misma, con un tama帽o de paso concreto, tendiendo en cuenta que:\n",
    "\n",
    "- los parches cuya suma de valores sean cero s贸lo deber谩n de ser incluidos sin `include_empty_patches` es verdadero.\n",
    "- s贸lo se incluir谩n parches completos (es decir, de tama帽o `patch_size` $\\times$ `patch_size`)\n",
    "\n",
    "Por ejemplo, para la siguiente imagen:\n",
    "\n",
    "           img= [[5 8 2 4 1 4 6 8 0 6 1]\n",
    "                 [3 8 1 4 3 5 6 9 3 1 7]\n",
    "                 [8 7 3 4 2 7 6 0 9 3 8]\n",
    "                 [4 7 3 2 7 2 4 7 5 0 5]\n",
    "                 [9 8 7 6 5 1 8 7 0 6 4]\n",
    "                 [0 3 8 4 7 0 0 3 5 5 2]\n",
    "                 [5 4 3 2 1 0 0 8 8 2 8]\n",
    "                 [8 3 7 8 2 5 0 3 8 2 4]\n",
    "                 [6 7 2 8 0 0 1 7 5 4 8]]\n",
    "         \n",
    "la ejecuci贸n de `extract_patches(img, 2,5)` da tres parches (el parche con cuatro 0's se excluye por `include_empty_patches=False`):\n",
    "\n",
    "        [[5 8]   [[4 6]   [[0 3]\n",
    "         [3 8]]   [5 6]]   [5 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches(img, patch_size, step_size, include_empty_patches=False):\n",
    "    patches = []\n",
    "    for y in range(0, img.shape[0]-patch_size+1, step_size):\n",
    "        for x in ...:\n",
    "            patch = ...\n",
    "    return patches\n",
    "\n",
    "import urllib, inspect\n",
    "src1 = urllib.quote_plus(inspect.getsource(extract_patches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprueba tu c贸digo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.array([[5, 8, 2, 4, 1, 4, 6, 8, 0, 6, 1],\n",
    "       [3, 8, 1, 4, 3, 5, 6, 9, 3, 1, 7],\n",
    "       [8, 7, 3, 4, 2, 7, 6, 0, 9, 3, 8],\n",
    "       [4, 7, 3, 2, 7, 2, 4, 7, 5, 0, 5],\n",
    "       [9, 8, 7, 6, 5, 1, 8, 7, 0, 6, 4],\n",
    "       [0, 3, 8, 4, 7, 0, 0, 3, 5, 5, 2],\n",
    "       [5, 4, 3, 2, 1, 0, 0, 8, 8, 2, 8],\n",
    "       [8, 3, 7, 8, 2, 5, 0, 3, 8, 2, 4],\n",
    "       [6, 7, 2, 8, 0, 0, 1, 7, 5, 4, 8]])\n",
    "for i in extract_patches(img, 2,5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa los parches extra铆dos de una imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size, step_size = 7, 2\n",
    "\n",
    "p = extract_patches(X[1307].reshape(28,28), patch_size, step_size, include_empty_patches=True)\n",
    "print \"patch extraction with empty patches\", len(p)\n",
    "pe = extract_patches(X[1307].reshape(28,28), patch_size, step_size)\n",
    "print \"without empty patches\", len(pe)\n",
    "plt.figure(figsize=(5,5))\n",
    "s = np.sqrt(len(p))\n",
    "for i in range(len(p)):\n",
    "    plt.subplot(s,s,i+1)\n",
    "    plt.imshow(p[i], cmap = plt.cm.Greys_r, interpolation=\"nearest\")\n",
    "    plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 5: Diccionario visual\n",
    "\n",
    "F铆jate c贸mo funciona KMeans. Trata de entender el c贸digo y ejec煤talo varias veces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "Xp, _ = make_blobs(n_samples=200,n_features=2, centers=4, cluster_std=2)\n",
    "cols = [\"red\", \"blue\", \"green\", \"gray\"]\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(121)\n",
    "plt.scatter(Xp[:,0], Xp[:,1], color=\"gray\")\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.subplot(122)\n",
    "plt.scatter(Xp[:,0], Xp[:,1])\n",
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=3)\n",
    "km.fit(Xp)\n",
    "y = km.predict(Xp)\n",
    "for i in np.unique(y):\n",
    "    plt.scatter(Xp[y==i][:,0], Xp[y==i][:,1], color=cols[i])\n",
    "    plt.xticks([]); plt.yticks([]);\n",
    "for i,p in enumerate(km.cluster_centers_):\n",
    "    plt.scatter(p[0], p[1], s=100, c=\"black\", marker=\"x\", lw=5)\n",
    "    plt.xticks([]); plt.yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer un KMeans con los parches extra铆dos de las im谩genes y consideraremos los centroides como palabras visuales. Para ello, completa la funci贸n siguiente de forma que:\n",
    "\n",
    "- tendr谩s que usar la funci贸n `extract_patches` del ejercicio anterior.\n",
    "- construye una lista con todos parches de las im谩genes de la matriz `X`. Observa que en cada file de la matriz hay una imagen linearizada y tendr谩s que hacer un `.reshape(28,28)` para convertirla en una matriz antes de llamar a `extract_patches`.\n",
    "- devuelve la lista de centroides del `KMeans`. Consulta la documentaci贸n de [KMeans en sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html).\n",
    "\n",
    "Esta lista de centroides ser谩 nuestro **diccionario visual** y, como cada centroide tiene el mismo tama帽o que los parches, los podremos visualizar. A continuaci贸n, completa el c贸digo de tal forma que se pueda visualizar el diccionario de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_visual_dictionary(X, patch_size, step_size, dict_size):\n",
    "    from sklearn.cluster import KMeans\n",
    "    patches = []\n",
    "    for img in X:\n",
    "        patches_in_this_image = ...\n",
    "        patches.append(...)\n",
    "\n",
    "    cinit = np.zeros((dict_size, patch_size**2))\n",
    "    km = KMeans(n_clusters=..., init=cinit, n_init=1)#, n_jobs=24)\n",
    "    km.fit(...)\n",
    "    return ...\n",
    "\n",
    "import urllib, inspect\n",
    "src2 = urllib.parse.quote_plus(inspect.getsource(get_visual_dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprueba tu c贸digo. la siguiente configuraci贸n te deber铆a de dar un conjunto de palabras visuales como este:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='imgs/vwords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size, step_size, dict_size = 7, 7, 30\n",
    "vwords = get_visual_dictionary(X[:100], patch_size, step_size, dict_size)\n",
    "plt.figure(figsize=(15,1.5))\n",
    "vwords = vwords[np.argsort([np.sum(vwords[i]) for i in range(len(vwords))])]\n",
    "print(np.sum([np.sum(vwords[i]) for i in range(len(vwords))]))\n",
    "n = len(vwords)\n",
    "for i in range(n):\n",
    "    plt.subplot(1,n,i+1)\n",
    "    plt.imshow(vwords[i].reshape(patch_size,patch_size),cmap = plt.cm.Greys_r,interpolation=\"nearest\")\n",
    "    plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 6: Histograma de palabras visuales\n",
    "\n",
    "Vamos ahora a calcular de qu茅 palabras visuales consta cada imagen. Esta ser谩 la representaci贸n _Bag of Features_ ya que, para cada palabra visual, contaremos cuantas veces aparece en cada imagen. Esto es an谩logo a contar, en un documento, cuantas veces aparece cada palabra.\n",
    "\n",
    "Para ello haremos dos funciones:\n",
    "\n",
    "- `get_closest`: en la que dado un parche y un diccionario visual como el devuelto en el ejercicio anterior nos de la palabara visual m谩s parecida. Esta similitud estar谩 medida en t茅rminos de la norma L2 (`np.linalg.norm`)\n",
    "\n",
    "- `get_histogram`:  que, dada una imagen y un diccionario, (1) extrae todos los parches de la imagen, (2) para cada parche obtiene cual es su palabra visual m谩s similar y (3) devuelve un vector con tantos elementos como palabras visuales, con la frecuencia relativa de aparici贸n de cada palabra visual en la imagen. La frecuencia relativa de una palabra visual $w$ viene dada por el n煤mero de parches de la imagen cuya palabra visual m谩s similar es $w$, dividido por el n煤mero total de parches.\n",
    "\n",
    "Completa el c贸digo de ambas funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest(patch, dictionary):\n",
    "    r = ...\n",
    "    return r\n",
    "\n",
    "def get_histogram(img, patch_size, step_size, dictionary):\n",
    "\n",
    "    histogram = ...\n",
    "    return histogram\n",
    "\n",
    "import urllib, inspect\n",
    "src3 = urllib.parse.quote_plus(inspect.getsource(get_closest)+inspect.getsource(get_histogram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprueba tu c贸digo. La celda siguiente selecciona un parche al azar de una imagen al azar y muestra su palabra visual m谩s cercana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size, step_size = 7, 2\n",
    "\n",
    "mnist = pd.read_csv(\"data/mnist1.5k.csv.gz\", compression=\"gzip\", header=None).values.astype(float)\n",
    "X=mnist[:,1:785]\n",
    "y=mnist[:,0]\n",
    "img = X[np.random.randint(len(X))].reshape(28,28)\n",
    "patches = extract_patches(img, patch_size, step_size)\n",
    "patch   = patches[np.random.randint(len(patches))].reshape(patch_size**2)\n",
    "vword = vwords[get_closest(patch, vwords)]\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.subplot(121)\n",
    "plt.imshow(patch.reshape(patch_size, patch_size),  cmap = plt.cm.Greys_r, interpolation=\"nearest\")\n",
    "plt.title(\"patch\")\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.subplot(122)\n",
    "plt.title(\"visual word\")\n",
    "plt.imshow(vword.reshape(patch_size, patch_size),  cmap = plt.cm.Greys_r, interpolation=\"nearest\")\n",
    "plt.xticks([]); plt.yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente celda selecciona una imagen al azar y obtiene su histograma de palabras visuales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.random.randint(len(X))\n",
    "print k\n",
    "img = X[k].reshape(28,28)\n",
    "h = get_histogram(img, patch_size, step_size, vwords)\n",
    "plt.imshow(img, cmap = plt.cm.Greys_r, interpolation=\"nearest\")\n",
    "\n",
    "plt.figure(figsize=(17,3))\n",
    "n = len(vwords)\n",
    "for i in range(n):\n",
    "    plt.subplot(1,n,i+1)\n",
    "    plt.imshow(vwords[i].reshape(patch_size,patch_size),cmap = plt.cm.Greys_r)\n",
    "    plt.title(\"%.2f\"%h[i])\n",
    "    plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimenta con la nueva representaci贸n\n",
    "\n",
    "Observa c贸mo aumenta el porcentaje de acierto representando las im谩genes con un histograma de palabras visuales. Este proceso puede demorarse varios minutos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size, step_size = 7,2\n",
    "#vdict = get_visual_dictionary(X, patch_size, step_size, 60)\n",
    "Xh = []\n",
    "for i, img in enumerate(X):\n",
    "    print(\"\\r\",i,)\n",
    "    Xh.append(get_histogram(img.reshape(28,28), patch_size, step_size, vdict))\n",
    "    \n",
    "Xh = np.array(Xh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "estimator = GaussianNB()\n",
    "sc = cross_val_score(estimator, X, y, cv=5)\n",
    "print(\"Original pixels                %.3f +/- %.3f\"%(np.mean(sc), np.std(sc)))\n",
    "sc = cross_val_score(estimator, Xh, y, cv=5)\n",
    "print(\"Bag of features representation %.3f +/- %.3f\"%(np.mean(sc), np.std(sc)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
