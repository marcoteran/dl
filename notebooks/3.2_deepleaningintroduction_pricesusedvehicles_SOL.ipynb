{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iFOX_Y3igEV"
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marcoteran/deeplearning/blob/master/notebooks/3.2_deepleaningintroduction_dnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Abrir en Colab\" title=\"Abrir y ejecutar en Google Colaboratory\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/marcoteran/deeplearning/blob/master/notebooks/3.2_deepleaningintroduction_dnn.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Abrir en Kaggle\" title=\"Abrir y ejecutar en Kaggle\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de c贸digo\n",
    "# Sesi贸n 08: Proyecto Precio de veh铆culos usados\n",
    "## Deep Learning y series de tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** Marco Teran **E-mail:** marco.tulio.teran@gmail.com,\n",
    "[Website](http://marcoteran.github.io/),\n",
    "[Github](https://github.com/marcoteran),\n",
    "[LinkedIn](https://www.linkedin.com/in/marcoteran/).\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VfqD089-WuQ"
   },
   "source": [
    "<h1 id=\"problema\">Contexto anal铆tico y exploraci贸n de datos</h1>\n",
    "\n",
    "El mercado de autos usados es reconocido por ser un sector economico muy competido con un centenar de compa帽ias que luchan por hacerse con una porci贸n de la torta. El precio de los autos se devalua a帽o a帽o debido a multiples factores y determinar el precio correcto es clave para las compa帽ias para lograr competir en el mercado. En este caso se requiere implementar una red neural que permita determinar el valor m谩s justo para los vehiculos dependiento de sus atributos.\n",
    "\n",
    "Se cuenta con un dataset (Craiglist_Cars.csv) que ser谩n cargados directamente a Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MdkkRVpVZbyQ"
   },
   "outputs": [],
   "source": [
    "#Importamos las librerias necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "#from google.colab import files #Librer铆a necesaria para interactuar con archivos en Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir data\n",
    "#!wget -O data/Craiglist_Cars.csv https://github.com/marcoteran/deeplearning/raw/master/notebooks/data/Craiglist_Cars.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "lUUfMdJ3rnXz",
    "outputId": "409b9ab7-c661-433b-ec6f-619c2246ef87"
   },
   "outputs": [],
   "source": [
    "#uploaded = files.upload()\n",
    "#cars = pd.read_csv(io.StringIO(uploaded['Craiglist_Cars.csv'].decode('utf-8')), sep = ',' )\n",
    "cars = pd.read_csv('data/Craiglist_Cars.csv', sep = ',' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSWTI9p6ZH9O"
   },
   "source": [
    "# 1. Evalua la cantidad, tipo y completitud de las variables disponibles\n",
    "\n",
    "En esta secci贸n, se realiza una exploraci贸n b谩sica del conjunto de datos \"cars\" utilizando algunas funciones de pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, se utiliza la funci贸n shape de pandas para imprimir la cantidad de filas y columnas en \"cars\". Esto proporciona una idea inicial de la magnitud del conjunto de datos.\n",
    "\n",
    "Luego, se utiliza la funci贸n isnull para detectar la cantidad de valores nulos en cada columna de \"cars\". Se calcula el porcentaje de valores nulos para cada columna y se imprime en pantalla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "B6Lu6dsRBOob",
    "outputId": "5f105ad3-9cc6-4ae8-cbb7-188c545f9937"
   },
   "outputs": [],
   "source": [
    "print(cars.shape)\n",
    "100*cars.isnull().sum()/cars.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuaci贸n, se utiliza la funci贸n DataFrame de pandas para crear un nuevo dataframe llamado \"types\", que almacena los tipos de datos de cada columna en \"cars\". Esto ayuda a comprender mejor la estructura de los datos y c贸mo se deben manejar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "8cYAAJxXBr2b",
    "outputId": "eedb0f65-dac2-4805-9b97-4c4bc1c97da0"
   },
   "outputs": [],
   "source": [
    "types = pd.DataFrame(cars.dtypes)\n",
    "print(types.groupby(0).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se crea una lista llamada \"categoricas\" que almacena los nombres de las columnas que contienen datos categ贸ricos (es decir, no num茅ricos). Se utiliza un bucle para recorrer cada columna categ贸rica y se imprime en pantalla la cantidad de valores 煤nicos en esa columna. Esto proporciona informaci贸n adicional sobre la naturaleza de los datos y c贸mo se deben procesar para el an谩lisis posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "07gsnrCtHL7e",
    "outputId": "af2392af-a6be-4787-fbb8-80028d5a4bce"
   },
   "outputs": [],
   "source": [
    "categoricas = types.index[types[0] == 'O'].values\n",
    "for line in categoricas:\n",
    " print(\"La variable \"+ line +\" contiene:\",str(len(cars[line].unique()))+\" distinct values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9_dNCTum1Hw"
   },
   "source": [
    "# Ingenier铆a de datos\n",
    "\n",
    "Ahora se debe preparar la informaci贸n para poder alimentar la red neuronal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRCjvmPz63n7"
   },
   "source": [
    "# 2. Implementa estrateg铆as para tratar la informaci贸n nula en las variables cuya tasa de nulos sea m谩ximo el 10%\n",
    "\n",
    "En esta secci贸n, se realizan algunas tareas de limpieza y preparaci贸n de datos para el conjunto de datos \"cars\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. En primer lugar, se rellenan los valores faltantes en la columna \"fuel\" con el valor m谩s com煤n utilizando la funci贸n fillna de pandas. Este es un ejemplo de c贸mo manejar valores nulos o faltantes en el conjunto de datos.\n",
    "2. Luego, se vuelven a imprimir las dimensiones de \"cars\" y se verifica si hay valores nulos en otras columnas.\n",
    "3. Despu茅s, se rellenan los valores faltantes en las columnas \"title_status\", \"transmission\" y \"manufacturer\" con el valor m谩s com煤n utilizando la funci贸n fillna de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hVNrQiqFO78K"
   },
   "outputs": [],
   "source": [
    "cars[\"fuel\"] = cars[\"fuel\"].fillna(cars[\"fuel\"].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "OK1S1PqgHgN6",
    "outputId": "d740ef8a-1c12-435f-a58e-d6848b4b210c"
   },
   "outputs": [],
   "source": [
    "print(cars.shape)\n",
    "100*cars.isnull().sum()/cars.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "XonBwc4GHkox",
    "outputId": "24cb0b42-dd3d-42f4-8a59-a4b2b1b200c5"
   },
   "outputs": [],
   "source": [
    "cars[\"title_status\"] = cars[\"title_status\"].fillna(cars[\"title_status\"].mode()[0])\n",
    "cars[\"transmission\"] = cars[\"transmission\"].fillna(cars[\"transmission\"].mode()[0])\n",
    "cars[\"manufacturer\"] = cars[\"manufacturer\"].fillna(cars[\"manufacturer\"].mode()[0])\n",
    "#Verificamos el cambio\n",
    "100*cars.isnull().sum()/cars.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtshbEQa7aRW"
   },
   "source": [
    "Luego del procedimiento anterior se debe proceder a convertir las variables categoricas en variables numericas. Durante el curso implementamos un m茅todo de One Hot Encoding disponible en Scikit Learn. En este caso utilizaremos una funcionalidad embedida en Pandas denominada [\"get_dummies\"](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A continuaci贸n, se crea una copia del dataframe llamado \"df\" y se aplican t茅cnicas de codificaci贸n de variables categ贸ricas. Se crea una nueva columna para cada valor posible en cada variable categ贸rica y se codifica como 1 si la observaci贸n tiene ese valor y 0 si no lo tiene. Esto se realiza utilizando la funci贸n get_dummies de pandas.\n",
    "* Adem谩s, se eliminan las columnas que contienen la categor铆a \"other\" ya que no aportan ning煤n valor al conjunto de datos.\n",
    "* Luego, se vuelven a imprimir las dimensiones de \"df\" y se imprime una vista previa del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "ZEtEYD5DS1iI",
    "outputId": "9daa5808-1aa1-4bd2-a268-de006183eb77"
   },
   "outputs": [],
   "source": [
    "df= cars.copy()\n",
    "for col in categoricas:\n",
    "    df = pd.concat([df, (pd.get_dummies(df[col])).astype(int)], axis=1)\n",
    "    df.drop(columns=[col],inplace=True)\n",
    "# Al crear las variables dummies se crean varias columnas referentes a categorias\n",
    "# 'other' que no aportan ning煤n valor al dataset por lo cual las eliminamos\n",
    "df.drop('other', axis=1, inplace=True)\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuaci贸n, se utiliza la funci贸n DataFrame de pandas para crear un nuevo dataframe llamado \"types\", que almacena los tipos de datos de cada columna en \"df\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "5Iesa56hVz8s",
    "outputId": "7ece6f52-e24c-4c20-93a5-ba8b4268006f"
   },
   "outputs": [],
   "source": [
    "types = pd.DataFrame(df.dtypes)\n",
    "print(\"Tipos de variables\",types.groupby(0).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despu茅s, se seleccionan las columnas num茅ricas del conjunto de datos y se eliminan las que corresponden a la variable objetivo (\"price\"). Esto se realiza utilizando la funci贸n set de Python para encontrar las columnas num茅ricas y luego la funci贸n list para convertir el resultado en una lista.\n",
    "\n",
    "Por 煤ltimo, se crea un nuevo dataframe llamado \"variables_consolidadas\" que contiene solo las variables num茅ricas y se crea un nuevo dataframe llamado \"objetivo\" que contiene solo la variable objetivo (\"price\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "d-Isxxj2wzPX",
    "outputId": "3396f906-287d-46d8-8656-afacd029b81a"
   },
   "outputs": [],
   "source": [
    "numeric_columns = list(set(types.index[types[0] ==\"int64\"].values) - set([\"price\"]))\n",
    "variables_consolidadas = df[numeric_columns]\n",
    "objetivo = df[\"price\"] #Variable objetivo de nuestra regresion.\n",
    "\n",
    "variables_consolidadas.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOP0z86EKrly"
   },
   "source": [
    "# 3. Separa el set de datos consolidados en 3 sets (entrenamiento, prueba y validaci贸n) de acuerdo con las recomendaciones vistas en el curso.\n",
    "\n",
    "\n",
    "En estas l铆neas de c贸digo se utiliza la librer铆a Scikit-learn para dividir los datos en conjuntos de entrenamiento, validaci贸n y prueba. Se utiliza la funci贸n \"train_test_split\" para crear los tres conjuntos a partir de las variables consolidadas (x) y la variable objetivo (y).\n",
    "* Primero, se divide en 80% de entrenamiento y 20% de prueba.\n",
    "* Luego, se divide el conjunto de entrenamiento en 90% para entrenamiento y 10% para validaci贸n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coloca tu c贸digo aqu铆\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jug8ZISZQQ0X"
   },
   "outputs": [],
   "source": [
    "#80% train 20% test\n",
    "x_train,x_test, y_train,y_test = train_test_split(variables_consolidadas,objetivo,test_size=0.2, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IzWNixQ6ItBR"
   },
   "outputs": [],
   "source": [
    "#90% train 10% val\n",
    "x_train,x_val, y_train,y_val = train_test_split(x_train,y_train,test_size=0.1, random_state=2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente, se utiliza la funci贸n \"reshape\" para cambiar la forma de las variables objetivo y poder utilizarlas en modelos de aprendizaje autom谩tico. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rWpp1VekItJi"
   },
   "outputs": [],
   "source": [
    "y_train=y_train.values.reshape(-1,1)\n",
    "y_test=y_test.values.reshape(-1,1)\n",
    "y_val=y_val.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se imprimen las formas (shapes) de los conjuntos de entrenamiento, validaci贸n y prueba para verificar que se hayan creado correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "nNaA2I2YItNJ",
    "outputId": "67cb9f31-3fbf-4b42-bb85-3d5ff8483576"
   },
   "outputs": [],
   "source": [
    "print(\"Shape of x_train:\",x_train.shape)\n",
    "print(\"Shape of x_test:\",x_test.shape)\n",
    "print(\"Shape of x_val:\",x_val.shape)\n",
    "print(\"Shape of y_train:\",y_train.shape)\n",
    "print(\"Shape of y_test:\",y_test.shape)\n",
    "print(\"Shape of y_val:\",y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APBt-AAfNBQj"
   },
   "source": [
    "<h1 id=\"arquitectura\">Dise帽o, Entrenamiento y Evaluaci贸n de la RN</h1>\n",
    "\n",
    "Una vez consolidado los sets de informaci贸n de entrenamiento, validacion y pruebas ya podemos iniciar a modelar nuestra red neuronal con las siguientes consideraciones:\n",
    "* Realiza la prueba con un par de arquitecturas iniciales.\n",
    "* Evalua el desempe帽o de la red.\n",
    "* Si el desempe帽o es bajo vuelve a la informaci贸n y prueba estrategias de estandarizaci贸n de la informaci贸n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "528iJ6fQ86ds"
   },
   "source": [
    "# 4. Implementa una red neuronal cuyas p茅rdidas (MSE) con el set de prueba sea menor a 0.40."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se importa la clase StandardScaler de la biblioteca sklearn.preprocessing, la cual es una t茅cnica de preprocesamiento de datos que escala los datos para que tengan una media de cero y una desviaci贸n est谩ndar de uno. Esto es importante porque ayuda a que el modelo pueda trabajar con variables en la misma escala, lo que puede mejorar su rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y6TWVkxGmcIB"
   },
   "outputs": [],
   "source": [
    "#Coloca tu c贸digo aqu铆\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea una instancia de StandardScaler() y se llama al m茅todo fit() con los datos de entrenamiento x_train, para que el objeto pueda aprender los par谩metros de escalamiento a partir de estos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utiliza el m茅todo transform() para escalar los conjuntos de datos de entrenamiento, validaci贸n y prueba x_train, x_val y x_test, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_scaled = scaler.transform(x_train)\n",
    "x_val_scaled = scaler.transform(x_val)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se repiten los pasos anteriores para la variable objetivo y_train, y_val y y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1 = StandardScaler()\n",
    "scaler1.fit(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_scaled = scaler1.transform(y_train)\n",
    "y_val_scaled = scaler1.transform(y_val)\n",
    "y_test_scaled = scaler1.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se importa la clase Sequential y los m贸dulos Dense, Dropout de la biblioteca Keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coloca tu c贸digo aqu铆\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, se crea una instancia de la clase Sequential y se a帽aden varias capas Dense con activaci贸n relu, una capa Dropout y una capa final con activaci贸n linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "5WORHuhPQmKX",
    "outputId": "73356a77-55e9-4bdc-f3c4-b3d72ea1013b"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256,input_dim = x_train.shape[1],activation=\"relu\"))\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1,activation = \"linear\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se compila el modelo con el optimizador adam, la funci贸n de p茅rdida mse (mean squared error) y la m茅trica mean_absolute_error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = \"adam\",loss=\"mse\",metrics=[\"mean_absolute_error\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se imprime un resumen del modelo con la funci贸n summary()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pydot\n",
    "#!pip install graphviz\n",
    "#!conda install -c anaconda pydot=1.2.3\n",
    "#!conda install -c anaconda pyparsing=2.2.0\n",
    "#!conda install GraphViz\n",
    "\n",
    "import errno\n",
    "import pydot\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muestra el diagrama del modelo en un archivo PNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se entrena el modelo llamando al m茅todo fit() con los datos escalados y se guardan los resultados del entrenamiento en el objeto modelhistory. El modelo se entrena durante 50 茅pocas con un tama帽o de lote de 1024. Tambi茅n se proporcionan los datos de validaci贸n para que se eval煤e el rendimiento del modelo en cada 茅poca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "H-y9uDZh_gxS",
    "outputId": "1faf6f2b-c3ca-4b1e-d144-abd2988050ff"
   },
   "outputs": [],
   "source": [
    "modelhistory=model.fit(x_train_scaled,y_train_scaled, validation_data = (x_val_scaled,y_val_scaled),epochs=50, batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubDSL59pCcBg"
   },
   "source": [
    "Ahora realiza la evaluaci贸n del modelo con el set de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "B6C_XX0dRHxR",
    "outputId": "4040b85f-8f2f-48d7-b35d-57ec73ed49ec"
   },
   "outputs": [],
   "source": [
    "result = model.evaluate(x_test_scaled,y_test_scaled)\n",
    "for i in range(len(model.metrics_names)):\n",
    "    print(\"Metric \",model.metrics_names[i],\":\", str(round(result[i],2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owT0Fwf0_N3i"
   },
   "source": [
    "Si el modelo cumple con el requerimiento, se guarda con el siguiente comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsuyXASg-aS0"
   },
   "outputs": [],
   "source": [
    "model.save('predictedprices.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6Qd8SigDTs3"
   },
   "source": [
    "# 5. Realiza un gr谩fico que evidencia la evoluci贸n de la funci贸n de p茅rdidas a traves de las distintas 茅pocas de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelhistory.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "FigeTaWgRqow",
    "outputId": "2332345e-99aa-4748-a8c7-ae245174a5a1"
   },
   "outputs": [],
   "source": [
    "#Coloca tu c贸digo aqu铆\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(13,6))\n",
    "plt.plot(modelhistory.history['loss'])\n",
    "plt.plot(modelhistory.history['val_loss'])\n",
    "plt.title(\"P茅rdidas del modelo con set de entrenamiento y pruebas por 茅poca\")\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('pocas')\n",
    "plt.legend(['Entrenamiento', 'Validaci贸n'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xE5rdI5bFG2J"
   },
   "source": [
    "Trata de realizar predicciones con el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "ZzoagaX5FELl",
    "outputId": "851a288a-11cd-4759-e4fa-b5ffd9a1089c"
   },
   "outputs": [],
   "source": [
    "real=pd.DataFrame(y_train)\n",
    "predic=model.predict(pd.DataFrame(x_train_scaled))\n",
    "valores_reescalados = scaler1.inverse_transform(predic)\n",
    "pred_escal =pd.DataFrame(valores_reescalados)\n",
    "# Muestra los valores reales y las predicciones\n",
    "for i in range(0,5):\n",
    "\tprint(\"Real=%s, Prediccion=%s\" % (real[0][i], pred_escal[0][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNB6p9Kx9lFV"
   },
   "source": [
    "___\n",
    "隆Todo bien! 隆Es todo por hoy! "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "Proyecto_Precios_Vehiculos_Usados_BLANCO.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
