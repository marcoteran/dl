{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iFOX_Y3igEV"
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marcoteran/deeplearning/blob/master/notebooks/3.2_deepleaningintroduction_dnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Abrir en Colab\" title=\"Abrir y ejecutar en Google Colaboratory\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/marcoteran/deeplearning/blob/master/notebooks/3.2_deepleaningintroduction_dnn.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Abrir en Kaggle\" title=\"Abrir y ejecutar en Kaggle\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de código\n",
    "# Sesión 08: Proyecto Precio de vehículos usados\n",
    "## Deep Learning y series de tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** Marco Teran **E-mail:** marco.tulio.teran@gmail.com,\n",
    "[Website](http://marcoteran.github.io/),\n",
    "[Github](https://github.com/marcoteran),\n",
    "[LinkedIn](https://www.linkedin.com/in/marcoteran/).\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VfqD089-WuQ"
   },
   "source": [
    "<h1 id=\"problema\">Contexto analítico y exploración de datos</h1>\n",
    "\n",
    "El mercado de autos usados es reconocido por ser un sector economico muy competido con un centenar de compañias que luchan por hacerse con una porción de la torta. El precio de los autos se devalua año año debido a multiples factores y determinar el precio correcto es clave para las compañias para lograr competir en el mercado. En este caso se requiere implementar una red neural que permita determinar el valor más justo para los vehiculos dependiento de sus atributos.\n",
    "\n",
    "Se cuenta con un dataset (Craiglist_Cars.csv) que serán cargados directamente a Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MdkkRVpVZbyQ"
   },
   "outputs": [],
   "source": [
    "#Importamos las librerias necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "#from google.colab import files #Librería necesaria para interactuar con archivos en Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir data\n",
    "#!wget -O data/Craiglist_Cars.csv https://github.com/marcoteran/deeplearning/raw/master/notebooks/data/Craiglist_Cars.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "lUUfMdJ3rnXz",
    "outputId": "409b9ab7-c661-433b-ec6f-619c2246ef87"
   },
   "outputs": [],
   "source": [
    "#uploaded = files.upload()\n",
    "#cars = pd.read_csv(io.StringIO(uploaded['Craiglist_Cars.csv'].decode('utf-8')), sep = ',' )\n",
    "cars = pd.read_csv('data/Craiglist_Cars.csv', sep = ',' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSWTI9p6ZH9O"
   },
   "source": [
    "# 1. Evalua la cantidad, tipo y completitud de las variables disponibles\n",
    "\n",
    "En esta sección, se realiza una exploración básica del conjunto de datos \"cars\" utilizando algunas funciones de pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, se utiliza la función shape de pandas para imprimir la cantidad de filas y columnas en \"cars\". Esto proporciona una idea inicial de la magnitud del conjunto de datos.\n",
    "\n",
    "Luego, se utiliza la función isnull para detectar la cantidad de valores nulos en cada columna de \"cars\". Se calcula el porcentaje de valores nulos para cada columna y se imprime en pantalla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "B6Lu6dsRBOob",
    "outputId": "5f105ad3-9cc6-4ae8-cbb7-188c545f9937"
   },
   "outputs": [],
   "source": [
    "print(cars.shape)\n",
    "100*cars.isnull().sum()/cars.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se utiliza la función DataFrame de pandas para crear un nuevo dataframe llamado \"types\", que almacena los tipos de datos de cada columna en \"cars\". Esto ayuda a comprender mejor la estructura de los datos y cómo se deben manejar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "8cYAAJxXBr2b",
    "outputId": "eedb0f65-dac2-4805-9b97-4c4bc1c97da0"
   },
   "outputs": [],
   "source": [
    "types = pd.DataFrame(cars.dtypes)\n",
    "print(types.groupby(0).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se crea una lista llamada \"categoricas\" que almacena los nombres de las columnas que contienen datos categóricos (es decir, no numéricos). Se utiliza un bucle para recorrer cada columna categórica y se imprime en pantalla la cantidad de valores únicos en esa columna. Esto proporciona información adicional sobre la naturaleza de los datos y cómo se deben procesar para el análisis posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "07gsnrCtHL7e",
    "outputId": "af2392af-a6be-4787-fbb8-80028d5a4bce"
   },
   "outputs": [],
   "source": [
    "categoricas = types.index[types[0] == 'O'].values\n",
    "for line in categoricas:\n",
    " print(\"La variable \"+ line +\" contiene:\",str(len(cars[line].unique()))+\" distinct values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9_dNCTum1Hw"
   },
   "source": [
    "# Ingeniería de datos\n",
    "\n",
    "Ahora se debe preparar la información para poder alimentar la red neuronal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRCjvmPz63n7"
   },
   "source": [
    "# 2. Implementa estrategías para tratar la información nula en las variables cuya tasa de nulos sea máximo el 10%\n",
    "\n",
    "En esta sección, se realizan algunas tareas de limpieza y preparación de datos para el conjunto de datos \"cars\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. En primer lugar, se rellenan los valores faltantes en la columna \"fuel\" con el valor más común utilizando la función fillna de pandas. Este es un ejemplo de cómo manejar valores nulos o faltantes en el conjunto de datos.\n",
    "2. Luego, se vuelven a imprimir las dimensiones de \"cars\" y se verifica si hay valores nulos en otras columnas.\n",
    "3. Después, se rellenan los valores faltantes en las columnas \"title_status\", \"transmission\" y \"manufacturer\" con el valor más común utilizando la función fillna de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hVNrQiqFO78K"
   },
   "outputs": [],
   "source": [
    "cars[\"fuel\"] = cars[\"fuel\"].fillna(cars[\"fuel\"].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "OK1S1PqgHgN6",
    "outputId": "d740ef8a-1c12-435f-a58e-d6848b4b210c"
   },
   "outputs": [],
   "source": [
    "print(cars.shape)\n",
    "100*cars.isnull().sum()/cars.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "XonBwc4GHkox",
    "outputId": "24cb0b42-dd3d-42f4-8a59-a4b2b1b200c5"
   },
   "outputs": [],
   "source": [
    "cars[\"title_status\"] = cars[\"title_status\"].fillna(cars[\"title_status\"].mode()[0])\n",
    "cars[\"transmission\"] = cars[\"transmission\"].fillna(cars[\"transmission\"].mode()[0])\n",
    "cars[\"manufacturer\"] = cars[\"manufacturer\"].fillna(cars[\"manufacturer\"].mode()[0])\n",
    "#Verificamos el cambio\n",
    "100*cars.isnull().sum()/cars.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtshbEQa7aRW"
   },
   "source": [
    "Luego del procedimiento anterior se debe proceder a convertir las variables categoricas en variables numericas. Durante el curso implementamos un método de One Hot Encoding disponible en Scikit Learn. En este caso utilizaremos una funcionalidad embedida en Pandas denominada [\"get_dummies\"](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A continuación, se crea una copia del dataframe llamado \"df\" y se aplican técnicas de codificación de variables categóricas. Se crea una nueva columna para cada valor posible en cada variable categórica y se codifica como 1 si la observación tiene ese valor y 0 si no lo tiene. Esto se realiza utilizando la función get_dummies de pandas.\n",
    "* Además, se eliminan las columnas que contienen la categoría \"other\" ya que no aportan ningún valor al conjunto de datos.\n",
    "* Luego, se vuelven a imprimir las dimensiones de \"df\" y se imprime una vista previa del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "ZEtEYD5DS1iI",
    "outputId": "9daa5808-1aa1-4bd2-a268-de006183eb77"
   },
   "outputs": [],
   "source": [
    "df= cars.copy()\n",
    "for col in categoricas:\n",
    "    df = pd.concat([df, (pd.get_dummies(df[col])).astype(int)], axis=1)\n",
    "    df.drop(columns=[col],inplace=True)\n",
    "# Al crear las variables dummies se crean varias columnas referentes a categorias\n",
    "# 'other' que no aportan ningún valor al dataset por lo cual las eliminamos\n",
    "df.drop('other', axis=1, inplace=True)\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se utiliza la función DataFrame de pandas para crear un nuevo dataframe llamado \"types\", que almacena los tipos de datos de cada columna en \"df\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "5Iesa56hVz8s",
    "outputId": "7ece6f52-e24c-4c20-93a5-ba8b4268006f"
   },
   "outputs": [],
   "source": [
    "types = pd.DataFrame(df.dtypes)\n",
    "print(\"Tipos de variables\",types.groupby(0).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después, se seleccionan las columnas numéricas del conjunto de datos y se eliminan las que corresponden a la variable objetivo (\"price\"). Esto se realiza utilizando la función set de Python para encontrar las columnas numéricas y luego la función list para convertir el resultado en una lista.\n",
    "\n",
    "Por último, se crea un nuevo dataframe llamado \"variables_consolidadas\" que contiene solo las variables numéricas y se crea un nuevo dataframe llamado \"objetivo\" que contiene solo la variable objetivo (\"price\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "d-Isxxj2wzPX",
    "outputId": "3396f906-287d-46d8-8656-afacd029b81a"
   },
   "outputs": [],
   "source": [
    "numeric_columns = list(set(types.index[types[0] ==\"int64\"].values) - set([\"price\"]))\n",
    "variables_consolidadas = df[numeric_columns]\n",
    "objetivo = df[\"price\"] #Variable objetivo de nuestra regresion.\n",
    "\n",
    "variables_consolidadas.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOP0z86EKrly"
   },
   "source": [
    "# 3. Separa el set de datos consolidados en 3 sets (entrenamiento, prueba y validación) de acuerdo con las recomendaciones vistas en el curso.\n",
    "\n",
    "\n",
    "En estas líneas de código se utiliza la librería Scikit-learn para dividir los datos en conjuntos de entrenamiento, validación y prueba. Se utiliza la función \"train_test_split\" para crear los tres conjuntos a partir de las variables consolidadas (x) y la variable objetivo (y).\n",
    "* Primero, se divide en 80% de entrenamiento y 20% de prueba.\n",
    "* Luego, se divide el conjunto de entrenamiento en 90% para entrenamiento y 10% para validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coloca tu código aquí\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jug8ZISZQQ0X"
   },
   "outputs": [],
   "source": [
    "#80% train 20% test\n",
    "x_train,x_test, y_train,y_test = train_test_split(variables_consolidadas,objetivo,test_size=0.2, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IzWNixQ6ItBR"
   },
   "outputs": [],
   "source": [
    "#90% train 10% val\n",
    "x_train,x_val, y_train,y_val = train_test_split(x_train,y_train,test_size=0.1, random_state=2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente, se utiliza la función \"reshape\" para cambiar la forma de las variables objetivo y poder utilizarlas en modelos de aprendizaje automático. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rWpp1VekItJi"
   },
   "outputs": [],
   "source": [
    "y_train=y_train.values.reshape(-1,1)\n",
    "y_test=y_test.values.reshape(-1,1)\n",
    "y_val=y_val.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se imprimen las formas (shapes) de los conjuntos de entrenamiento, validación y prueba para verificar que se hayan creado correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "nNaA2I2YItNJ",
    "outputId": "67cb9f31-3fbf-4b42-bb85-3d5ff8483576"
   },
   "outputs": [],
   "source": [
    "print(\"Shape of x_train:\",x_train.shape)\n",
    "print(\"Shape of x_test:\",x_test.shape)\n",
    "print(\"Shape of x_val:\",x_val.shape)\n",
    "print(\"Shape of y_train:\",y_train.shape)\n",
    "print(\"Shape of y_test:\",y_test.shape)\n",
    "print(\"Shape of y_val:\",y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APBt-AAfNBQj"
   },
   "source": [
    "<h1 id=\"arquitectura\">Diseño, Entrenamiento y Evaluación de la RN</h1>\n",
    "\n",
    "Una vez consolidado los sets de información de entrenamiento, validacion y pruebas ya podemos iniciar a modelar nuestra red neuronal con las siguientes consideraciones:\n",
    "* Realiza la prueba con un par de arquitecturas iniciales.\n",
    "* Evalua el desempeño de la red.\n",
    "* Si el desempeño es bajo vuelve a la información y prueba estrategias de estandarización de la información."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "528iJ6fQ86ds"
   },
   "source": [
    "# 4. Implementa una red neuronal cuyas pérdidas (MSE) con el set de prueba sea menor a 0.40."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se importa la clase StandardScaler de la biblioteca sklearn.preprocessing, la cual es una técnica de preprocesamiento de datos que escala los datos para que tengan una media de cero y una desviación estándar de uno. Esto es importante porque ayuda a que el modelo pueda trabajar con variables en la misma escala, lo que puede mejorar su rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y6TWVkxGmcIB"
   },
   "outputs": [],
   "source": [
    "#Coloca tu código aquí\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea una instancia de StandardScaler() y se llama al método fit() con los datos de entrenamiento x_train, para que el objeto pueda aprender los parámetros de escalamiento a partir de estos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utiliza el método transform() para escalar los conjuntos de datos de entrenamiento, validación y prueba x_train, x_val y x_test, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_scaled = scaler.transform(x_train)\n",
    "x_val_scaled = scaler.transform(x_val)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se repiten los pasos anteriores para la variable objetivo y_train, y_val y y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1 = StandardScaler()\n",
    "scaler1.fit(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_scaled = scaler1.transform(y_train)\n",
    "y_val_scaled = scaler1.transform(y_val)\n",
    "y_test_scaled = scaler1.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se importa la clase Sequential y los módulos Dense, Dropout de la biblioteca Keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coloca tu código aquí\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, se crea una instancia de la clase Sequential y se añaden varias capas Dense con activación relu, una capa Dropout y una capa final con activación linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "5WORHuhPQmKX",
    "outputId": "73356a77-55e9-4bdc-f3c4-b3d72ea1013b"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256,input_dim = x_train.shape[1],activation=\"relu\"))\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1,activation = \"linear\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se compila el modelo con el optimizador adam, la función de pérdida mse (mean squared error) y la métrica mean_absolute_error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = \"adam\",loss=\"mse\",metrics=[\"mean_absolute_error\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se imprime un resumen del modelo con la función summary()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pydot\n",
    "#!pip install graphviz\n",
    "#!conda install -c anaconda pydot=1.2.3\n",
    "#!conda install -c anaconda pyparsing=2.2.0\n",
    "#!conda install GraphViz\n",
    "\n",
    "import errno\n",
    "import pydot\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muestra el diagrama del modelo en un archivo PNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se entrena el modelo llamando al método fit() con los datos escalados y se guardan los resultados del entrenamiento en el objeto modelhistory. El modelo se entrena durante 50 épocas con un tamaño de lote de 1024. También se proporcionan los datos de validación para que se evalúe el rendimiento del modelo en cada época."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "H-y9uDZh_gxS",
    "outputId": "1faf6f2b-c3ca-4b1e-d144-abd2988050ff"
   },
   "outputs": [],
   "source": [
    "modelhistory=model.fit(x_train_scaled,y_train_scaled, validation_data = (x_val_scaled,y_val_scaled),epochs=50, batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubDSL59pCcBg"
   },
   "source": [
    "Ahora realiza la evaluación del modelo con el set de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "B6C_XX0dRHxR",
    "outputId": "4040b85f-8f2f-48d7-b35d-57ec73ed49ec"
   },
   "outputs": [],
   "source": [
    "result = model.evaluate(x_test_scaled,y_test_scaled)\n",
    "for i in range(len(model.metrics_names)):\n",
    "    print(\"Metric \",model.metrics_names[i],\":\", str(round(result[i],2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owT0Fwf0_N3i"
   },
   "source": [
    "Si el modelo cumple con el requerimiento, se guarda con el siguiente comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsuyXASg-aS0"
   },
   "outputs": [],
   "source": [
    "model.save('predictedprices.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6Qd8SigDTs3"
   },
   "source": [
    "# 5. Realiza un gráfico que evidencia la evolución de la función de pérdidas a traves de las distintas épocas de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelhistory.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "FigeTaWgRqow",
    "outputId": "2332345e-99aa-4748-a8c7-ae245174a5a1"
   },
   "outputs": [],
   "source": [
    "#Coloca tu código aquí\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(13,6))\n",
    "plt.plot(modelhistory.history['loss'])\n",
    "plt.plot(modelhistory.history['val_loss'])\n",
    "plt.title(\"Pérdidas del modelo con set de entrenamiento y pruebas por época\")\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Épocas')\n",
    "plt.legend(['Entrenamiento', 'Validación'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xE5rdI5bFG2J"
   },
   "source": [
    "Trata de realizar predicciones con el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "ZzoagaX5FELl",
    "outputId": "851a288a-11cd-4759-e4fa-b5ffd9a1089c"
   },
   "outputs": [],
   "source": [
    "real=pd.DataFrame(y_train)\n",
    "predic=model.predict(pd.DataFrame(x_train_scaled))\n",
    "valores_reescalados = scaler1.inverse_transform(predic)\n",
    "pred_escal =pd.DataFrame(valores_reescalados)\n",
    "# Muestra los valores reales y las predicciones\n",
    "for i in range(0,5):\n",
    "\tprint(\"Real=%s, Prediccion=%s\" % (real[0][i], pred_escal[0][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNB6p9Kx9lFV"
   },
   "source": [
    "___\n",
    "¡Todo bien! ¡Es todo por hoy! 😀"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "Proyecto_Precios_Vehiculos_Usados_BLANCO.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
